\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{balance}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=2pt,parsep=1pt,partopsep=1pt}
\usepackage{multirow}

\usepackage{xspace}
\newcommand{\ie}{{\emph{i.e.}},\xspace}
\newcommand{\viz}{{\emph{viz.}},\xspace}
\newcommand{\eg}{{\emph{e.g.}},\xspace}
\newcommand{\etc}{etc.}
\newcommand{\etal}{{\emph{et al.}}}
\newcommand{\GH}{{\sc GitHub}\xspace}
\newcommand{\DO}{{\sc DevOps}\xspace}
\newcommand{\GHT}{{\sc GHTorrent}\xspace}
\newcommand{\CI}{{$\mathcal{CI}$}\xspace}
\newcommand{\tdd}{{$\mathcal{TDD}$}\xspace}
\newcommand{\bCI}{{$\bm{\mathcal{CI}}$}\xspace}
\newcommand{\PR}{{$\mathcal{PR}$}\xspace}
\newcommand{\PRs}{{$\mathcal{PR}$s}\xspace}
\newcommand\head[1]{\noindent{\underline{\bf{#1}}:}}
\newcommand{\mypara}[1]{\vspace{4 mm} \noindent{\underline {\bf #1} \vspace{2mm}}}

\usepackage{amssymb}
\newcommand{\nnbb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }

\newcommand{\as}[1]{\nnbb{Alexander}{#1}}
\newcommand{\bv}[1]{\nnbb{Bogdan}{#1}}
\newcommand{\vf}[1]{\nnbb{Vladimir}{#1}}
\newcommand{\yz}[1]{\nnbb{Yangyang}{#1}}

\begin{document}

\title{Census of Software Engineering Practices
Following Adoption of Continuous Integration}

\author
{\IEEEauthorblockN{}
\IEEEauthorblockA{}
}
\maketitle
\begin{abstract}
Continuous Integration (CI) has become a key part of the modern ideology of mashing development and operations together to shorten the cycles of delivering a product to users. CI has become a disruptive innovation in software development: with proper implementation, e.g. Travis CI or Jenkins CI, and adoption, positive effects have been demonstrated on pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential. Here we study the adaptation and evolution of code writing, review practices and unit testing practices as Travis CI is adopted by hundreds of established projects on GitHub. By employing a mix of quantitative studies and case studies we triangulate the general evolution trajectories, and provide reasoning for the differences encountered among the projects.
\end{abstract}

\input{intro}


\section{Background and Theory}

For a developer not proficient with the operations side of the process, transitioning to an integrated CI platform, like Travis CI, involves adaptation of their established processes to the new environment. During this transition, some developers will experience a more streamlined process evolution trajectory than others. Studying those trajectories can provide lessons learned.


We expect the following to potentially change in the transition <need to write theory behind each>:

On the developer side:
Change in code writing/committing: we expect smaller change sets over time
We expect More unit testing over time
Operations side:
More discussions in code review over time
Different categories of initial faults

Continuous integration encourages developers to ``break down their work into small chunks of a few hours each'', as smaller and more frequent commits keep them to keep track of their progress and reduces the debug effort~\cite{Fowler,Duvall}. %Duvall [p. 31,38,40]
Therefore, in \textbf{RQ1} we investigate whether introduction of the continuous integration has indeed led to smaller commits.
\as{Why do not we look at their frequency? In an early study Miller has observed that on average Microsoft developers committed once a day, while off-shore developers committed less frequently due to network latencies~\cite{Miller}; Y\"{u}ksel reports 33 commits per day~\cite{Yuksel}.}

RQ1: Are developers reducing the size of code changes in each commit post CI adoption? Do they continue to do so over time?

Moreover, continuous integration is closely related to presence of automated tests~\cite{Fowler}. Duvall even claims that continuous integration without such tests should not be considered continuous integration at all~\cite{Duvall}, while Cannizzo, Clutton and Ramesh deem an extensive suite of unit and acceptance tests to be an essential first step~\cite{CannizzoCluttonRamesh}. \as{Y\"{u}ksel reports increase of the number of automated tests but they have combined introduction of CI with automated testing~\cite{Yuksel}. }

RQ2: How are developers transitioning to automated testing over time?

For continuos integration to have the stated benefits, code review should play a prominent role. In a pull-request model of development, code review is done through comments in open issues.

RQ3: Are developers transitioning to using more issues after the adoption of CI?


%\subsection{RQs}

\input{method}

\section{Results and Discussion}

We sought to study how different software development practices evolve around the time of CI adoption and the period after the adoption.
\input{examples}

\subsection{RQ1: Trends in Code Churn}

The first development practice we examine is code churn.
The data consists of 567 projects, each with at least 500 nonmerge commits.

\noindent \underline{Exploratory Study} To explore general trends over time, we first look at code churn in months leading up to CI adoption.
Fig.~\ref{Fig:CodeChurnBefore} shows a boxplot of per-project median code churn for each of nine consecutive 30-day time intervals before CI adoption.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.
We observe that for most part, the medians dance around 
10 and 12 lines of code churn per period, and the averages between 16 and 18 lines, with large variance and no obvious trend. The 30-day interval just before CI adoption seems to stand out and is higher than the rest.

Next, we look to the other side of the adoption point. Fig.~\ref{Fig:CodeChurnBefore} shows a boxplot of per-project median code churn for each of nine consecutive 30-day time intervals following CI adoption.
Here,  there seems to be a slight downward trend in both the medians and averages, the former drop monotonically from 13 down to 8 lines of code and the latter down to 15 lines of code.
However, we note the large variance around the medians.

The 30- and 60-day intervals just after the adoption point seem to be higher than the rest, and together with the 30-day interval just before adoption of CI seem to form a peak of increased code churn.
In conclusion, discounting the peak behavior in code churn around CI adoption time, we observe a small reduction in the amounts of code churn from before to after CI adoption.

\input{tables/rq1_models}
\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{churn_before.pdf}
\caption{The Code churn before CI adoption}
\label{Fig:CodeChurnBefore}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{churn_after.pdf}
\caption{The Code churn after CI adoption}
\label{Fig:CodeChurnAfter}
\end{figure}

\noindent \underline{Modeling Study} 
Guided by our observations from the exploratory study above, we proceed to quantify the trends we observed.
For each project, we fitted a sharp RDD model, as decribed before.
Table~\ref{Table:rddmodels} summarizes the results.


Table XX: Projects with statistical negative trends after CI

\noindent \underline{Discussion}
The dominant trend following CI adoption is a downward one, twice as frequent as the upward one.
This is consistent with Fowler's good practices of CI, of committing more frequently and smaller pieces of code.
Still, most projects end up not following a trend after their adoption of CI, while still remaining active.


The increased code churn on both sides near the CI adoption time is arguably in line with expectations that more maintanance work may be going on in preparation for the transition to CI, and that the projects go through some adjustment/cleanup period right after.

\noindent \underline{Case Study: Transition Period}


\subsection{RQ2: Trends in Commit Frequency}

The second development practice we examine is commit frequency.
We use the same data as before, 567 projects, each with at least 500 nonmerge commits.

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{numbercommits.pdf}
\caption{Commit frequency before and after CI adoption}
\label{Fig:NumberCommits}
\end{figure}

\noindent \underline{Exploratory Study} To explore general trends over time, we first look at commit frequency in the months leading up to CI adoption.
Figs.~\ref{Fig:NumberCommits} shows the boxplots of per-project median code churn for each of nine consecutive 30-day time intervals before CI adoption, and respectively, nine consecutive 30-day time intervals after CI adoption.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.


\noindent \underline{Modeling Study} 
As in RQ1, for each project, we fitted a sharp RDD model, as decribed before.
Table~\ref{Table:rddmodels_freq} summarizes the results.

\input{tables/rq2_models}


\noindent \underline{Discussion}




\subsection{RQ3: Trends in Issues}

Fig 5: Box plots of \# issues per unit time period, one for each time point, each point an aggregate of all projects

\input{tables/rq3_models}

The values of the $\gamma$'s for these models are right skewed, with average and median over 1. Thus, we conclude the effect of CI adoption led to an overall increase of one issue more per period.




\subsection{RQ4: Trends in Testing}

The second development practice we examine is testing and the evolution of the types of errors revealed by automated testing.
The data consists of XXX projects, each with at least YYY tests.

\noindent \underline{Exploratory Study} As before, we first look for general trends over time.
Fig.~\ref{Fig:Tests} shows a boxplot of per-project median number of tests per build, for each of five consecutive 60-day time intervals before CI adoption.
We aggregated the data here in 60 day intervals to make it easier to visualize the trend since the differences are small.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.
We observe a monotonically increasing trend in both the medians, from 140 to 160 tests per build, and the means, from 205 to 245 tests per build, i.e. 15\% to 20\% increase. The trend in the medians is significant statistically (which test?, p-val = 0.00?).


\noindent \underline{Discussion}


\input{rw}
\section{Conclusion}

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{issues_before.pdf}
\caption{Median number of issues before CI adoption}
\label{Fig:IssuesBefore}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{issues_after.pdf}
\caption{Median number of issues after CI adoption}
\label{Fig:IssuesAfter}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{tests.pdf}
\caption{Unit tests per build following CI adoption}
\label{Fig:Tests}
\end{figure}

\input{threats}

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{plot_together.png}
\caption{Evolution of Error Types Since CI Adoption}
\label{Fig:BugTypes}
\end{figure}

\balance

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}