\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{balance}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=2pt,parsep=1pt,partopsep=1pt}
\usepackage{multirow}

\usepackage{xspace}
\newcommand{\ie}{{\emph{i.e.}},\xspace}
\newcommand{\viz}{{\emph{viz.}},\xspace}
\newcommand{\eg}{{\emph{e.g.}},\xspace}
\newcommand{\etc}{etc.}
\newcommand{\etal}{{\emph{et al.}}}
\newcommand{\GH}{{\sc GitHub}\xspace}
\newcommand{\DO}{{\sc DevOps}\xspace}
\newcommand{\GHT}{{\sc GHTorrent}\xspace}
\newcommand{\CI}{{$\mathcal{CI}$}\xspace}
\newcommand{\tdd}{{$\mathcal{TDD}$}\xspace}
\newcommand{\bCI}{{$\bm{\mathcal{CI}}$}\xspace}
\newcommand{\PR}{{$\mathcal{PR}$}\xspace}
\newcommand{\PRs}{{$\mathcal{PR}$s}\xspace}
\newcommand\head[1]{\noindent{\underline{\bf{#1}}:}}
\newcommand{\mypara}[1]{\vspace{4 mm} \noindent{\underline {\bf #1} \vspace{2mm}}}

\usepackage[table]{xcolor}
\usepackage{ifthen}
\usepackage{amssymb}
\newboolean{showcomments}
\setboolean{showcomments}{true} % toggle to show or hide comments
\ifthenelse{\boolean{showcomments}}
  {\newcommand{\nnbb}[2]{
    \fcolorbox{gray}{yellow}{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }
   \newcommand{\version}{\emph{\scriptsize$-$working$-$}}
  }
  {\newcommand{\nb}[2]{}
   \newcommand{\version}{}
  }
\newcommand{\as}[1]{\nnbb{Alexander}{#1}}
\newcommand{\bv}[1]{\nnbb{Bogdan}{#1}}
\newcommand{\vf}[1]{\nnbb{Vladimir}{#1}}
\newcommand{\yz}[1]{\nnbb{Yangyang}{#1}}

\begin{document}

\title{How Do Software Engineering Practices
Change Following Adoption of Continuous Integration?}

\author
{\IEEEauthorblockN{Yangyang Zhao}
\IEEEauthorblockA{Nanjing University}
\and
\IEEEauthorblockN{Bogdan Vasilescu}
\IEEEauthorblockA{Carnegie Mellon University}
\and
\IEEEauthorblockN{Yuming Zhou}
\IEEEauthorblockA{Nanjing University}
\and
\IEEEauthorblockN{Alexander Serebrenik}
\IEEEauthorblockA{Eindhoven U of Technology}
\and
\IEEEauthorblockN{Vladimir Filkov}
\IEEEauthorblockA{UC Davis}
}
\maketitle
\begin{abstract}
Continuous Integration (CI) has become a key part of the modern ideology of mashing development and operations together to shorten the cycles of delivering a product to users. CI has become a disruptive innovation in software development: with proper implementation, e.g. Travis CI or Jenkins CI, and adoption, positive effects have been demonstrated on pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential. Here we study the adaptation and evolution of code writing, review practices and unit testing practices as Travis CI is adopted by hundreds of established projects on GitHub. By employing a mix of quantitative studies and case studies we triangulate the general evolution trajectories, and provide reasoning for the differences encountered among the projects.
\end{abstract}

\input{intro}


\section{Background and Theory}

For a developer not proficient with the operations side of the process, transitioning to an integrated CI platform, like Travis CI, involves adaptation of their established processes to the new environment. During this transition, some developers will experience a more streamlined process evolution trajectory than others. Studying those trajectories can provide lessons learned.


%We expect the following to potentially change in the transition <need to write theory behind each>:
%
%On the developer side:
%Change in code writing/committing: we expect smaller change sets over time
%We expect More unit testing over time
%Operations side:
%More discussions in code review over time
%Different categories of initial faults

Continuous integration encourages developers to ``break down their work into small chunks of a few hours each'', as smaller and more frequent commits keep them to keep track of their progress and reduces the debug effort~\cite{Fowler,Duvall}. %Duvall [p. 31,38,40]
Based on these guidelines we formulate our \textbf{RQ1}: Are developers reducing the size of code changes in each commit post CI adoption? Do they continue to do so over time?

Size of the commit is closely related to commit frequency: indeed, the aforementioned quote from Fowler's blog refers to ``chunks of a few hours each''. In an early study Miller has observed that on average Microsoft developers committed once a day, while off-shore developers committed less frequently due to network latencies~\cite{Miller}; Y\"{u}ksel reports 33 commits per day after introduction of CI~\cite{Yuksel}. 
Hence, we ask \textbf{RQ2}: Are developers committing code more frequently?
\as{stopped here}
For continuos integration to have the stated benefits, code review should play a prominent role. In a pull-request model of development, code review is done through comments in open issues.

RQ3: Are developers transitioning to using more issues after the adoption of CI?


Moreover, continuous integration is closely related to presence of automated tests~\cite{Fowler}. Duvall even claims that continuous integration without such tests should not be considered continuous integration at all~\cite{Duvall}, while Cannizzo, Clutton and Ramesh deem an extensive suite of unit and acceptance tests to be an essential first step~\cite{CannizzoCluttonRamesh}. \as{Y\"{u}ksel reports increase of the number of automated tests but they have combined introduction of CI with automated testing~\cite{Yuksel}. }

RQ4: How are developers transitioning to automated testing over time?

%\subsection{RQs}

\input{method}
\input{examples}

\section{Results and Discussion}

We sought to study how different software development practices evolve around the time of CI adoption and the period after the adoption.


\subsection{RQ1: Trends in Code Churn}

The first development practice we examine is code churn.
The data consists of 567 projects, each with at least 500 nonmerge commits.

\noindent \underline{Exploratory Study} To explore general trends over time, we first look at code churn in months leading up to CI adoption.
Fig.~\ref{Fig:CodeChurnBefore} shows a boxplot of per-project median code churn for each of nine consecutive 30-day time intervals before CI adoption.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.
We observe that for most part, the medians dance around 
10 and 12 lines of code churn per period, and the averages between 16 and 18 lines, with large variance and no obvious trend. The 30-day interval just before CI adoption seems to stand out and is higher than the rest.

Next, we look to the other side of the adoption point. Fig.~\ref{Fig:CodeChurnBefore} shows a boxplot of per-project median code churn for each of nine consecutive 30-day time intervals following CI adoption.
Here,  there seems to be a slight downward trend in both the medians and averages, the former drop monotonically from 13 down to 8 lines of code and the latter down to 15 lines of code.
However, we note the large variance around the medians.

The 30- and 60-day intervals just after the adoption point seem to be higher than the rest, and together with the 30-day interval just before adoption of CI seem to form a peak of increased code churn.
In conclusion, discounting the peak behavior in code churn around CI adoption time, we observe a small reduction in the amounts of code churn from before to after CI adoption.

\input{tables/rq1_models}
\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{churn_before.pdf}
\caption{The Code churn before CI adoption}
\label{Fig:CodeChurnBefore}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{churn_after.pdf}
\caption{The Code churn after CI adoption}
\label{Fig:CodeChurnAfter}
\end{figure}

\noindent \underline{Modeling Study} 
Guided by our observations from the exploratory study above, we proceed to quantify the trends we observed.
For each project, we fitted a sharp RDD model, as decribed before.
Because of the peak we observed, we eliminated the time point immediately before and the one immediately after the CI adoption time so as they do not throw off the regression models.
Table~\ref{Table:rddmodels} summarizes the results for the models on the remaining 16 time points.
Recall that $\beta$ is the slope before, $\gamma$ the size of the effect of CI introduction, $\delta$ is the divergence in the slopes before and after CI adoption, and $\beta + \delta$ the slope of the linear trend after CI adoption.

Of the 567 models, all but one could be fit to the data.
In 504 of the models, there was no significant linear relationship of code churn to time, i.e. there was no linear trend.
In 45 projects there was a rising trend before CI and in 17 projects a falling trend before CI.
Similarly, in 508 projects no effect of CI introduction on code churn was found. Among the 58 projects where an effect was significant, in about half (28) the trend was positive, i.e., code churn increased, and in the other half (30) it was negative, i.e., code churn decreased.
Interestingly, while in 514 projects no change of slope in the trends before and after CI adoption was found, in the 52 with significant change of trends a reduction in the slope was more prevalent than an increase by 4 to 1.
This carried over into trends following CI adoption: out of the 52 significant trend changes, 39 are downward trends, and only 13 are upward.

%Table XX: Projects with statistical negative trends after CI

\noindent \underline{Discussion}

Most of the projects could not be modeled with the RDD linear regression.
Some of the reasons for this are: dispersion of the data,
non-uniform distribution of the events across time, and project specific considerations that we could not model.

Our exploratory study showed a downward trend following CI adoption.
The modeling study showed that when linear trends were present, the downward ones after CI adoption were three times as frequent as the upward one, a reversal of the pattern in the trends before CI.
This is consistent with Fowler's good practices of CI, of committing more frequently and smaller pieces of code.
Still, most projects end up not following a trend after their adoption of CI, while still remaining active.

The increased code churn on both sides near the CI adoption time is arguably in line with expectations that more maintanance work may be going on in preparation for the transition to CI, and that the projects go through some adjustment/cleanup period right after.


\subsection{RQ2: Trends in Commit Frequency}

The second development practice we examine is commit frequency.
We use the same data as before, 567 projects, each with at least 500 nonmerge commits.

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{numbercommits.pdf}
\caption{Commit frequency before and after CI adoption}
\label{Fig:NumberCommits}
\end{figure}

\noindent \underline{Exploratory Study} To explore general trends over time, we first look at commit frequency in the months leading up to CI adoption.
Figs.~\ref{Fig:NumberCommits} shows the boxplots of per-project median code churn for each of nine consecutive 30-day time intervals before CI adoption, and respectively, nine consecutive 30-day time intervals after CI adoption.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.

As was the case with code churn, here we also see a peak in the 30-day interval just before and the one just after CI adoption.
Unlike for code churn, here we do not observe an apparent trend in the median or average commit frequency.

\noindent \underline{Modeling Study} 
As in RQ1, for each project, we fitted a sharp RDD model, as decribed before.
Table~\ref{Table:rddmodels_freq} summarizes the results.
We eliminated one time point on either side of the CI adoption time so as they do not throw off the regression models.
\input{tables/rq2_models}

We were able to fit all 567 models to the data.
In 458 of the models, there was no significant linear relationship of commit frequency to time, i.e. there was no linear trend.
In 67 projects there was a rising trend in commit frequency before CI and in 42 projects a falling trend before CI.
Similarly, in 476 projects no effect of CI introduction on commit frequency was found. Among the 91 projects where an effect was significant, in about 50 the trend was positive, i.e., commit frequency increased, and in the rest it was negative, i.e., commit frequency decreased.
Interestingly, while in 468 projects no change of slope in the trends before and after CI adoption was found, in the 99 with significant change of trends a reduction in the slope was more prevalent than an increase by about 2 to 1.
This carried over into trends in commit frequency following CI adoption: out of the 99 significant trend changes, 66 are downward trends, and only 33 are upward.

\noindent \underline{Discussion}




\subsection{RQ3: Trends in Issues}

\noindent \underline{Exploratory Study}


\noindent \underline{Modeling Study}
Fig 5: Box plots of \# issues per unit time period, one for each time point, each point an aggregate of all projects


\noindent \underline{Discussion}
\input{tables/rq3_models}

The values of the $\gamma$'s for these models are right skewed, with average and median over 1. Thus, we conclude the effect of CI adoption led to an overall increase of one issue more per period.




\subsection{RQ4: Trends in Testing}

The second development practice we examine is testing and the evolution of the types of errors revealed by automated testing.
The data consists of XXX projects, each with at least YYY tests.

\noindent \underline{Exploratory Study} As before, we first look for general trends over time.
Fig.~\ref{Fig:Tests} shows a boxplot of per-project median number of tests per build, for each of five consecutive 60-day time intervals before CI adoption.
We aggregated the data here in 60 day intervals to make it easier to visualize the trend since the differences are small.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.
We observe a monotonically increasing trend in both the medians, from 140 to 160 tests per build, and the means, from 205 to 245 tests per build, i.e. 15\% to 20\% increase. 

\noindent \underline{Error Types Study}

\noindent \underline{Discussion}


\input{rw}
\section{Conclusion}

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{issues_before.pdf}
\caption{Median number of issues before CI adoption}
\label{Fig:IssuesBefore}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{issues_after.pdf}
\caption{Median number of issues after CI adoption}
\label{Fig:IssuesAfter}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{tests.pdf}
\caption{Unit tests per build following CI adoption}
\label{Fig:Tests}
\end{figure}

\input{threats}

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{plot_together.png}
\caption{Evolution of Error Types Since CI Adoption}
\label{Fig:BugTypes}
\end{figure}

\balance

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}