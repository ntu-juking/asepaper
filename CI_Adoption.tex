\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{balance}
\usepackage{graphicx}

\usepackage{multirow}

\usepackage{xspace}
\newcommand{\ie}{{\emph{i.e.}},\xspace}
\newcommand{\viz}{{\emph{viz.}},\xspace}
\newcommand{\eg}{{\emph{e.g.}},\xspace}
\newcommand{\etc}{etc.}
\newcommand{\etal}{{\emph{et al.}}}

\usepackage{amssymb}
\newcommand{\nnbb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
   }

\newcommand{\as}[1]{\nnbb{Alexander}{#1}}
\newcommand{\bv}[1]{\nnbb{Bogdan}{#1}}
\newcommand{\vf}[1]{\nnbb{Vladimir}{#1}}
\newcommand{\yz}[1]{\nnbb{Yangyang}{#1}}

\begin{document}

\title{Census of Software Engineering Practices
Following Adoption of Continuous Integration}

\author
{\IEEEauthorblockN{}
\IEEEauthorblockA{}
}
\maketitle
\begin{abstract}
Continuous Integration (CI) has become a key part of the modern ideology of mashing development and operations together to shorten the cycles of delivering a product to users. CI has become a disruptive innovation in software development: with proper implementation, e.g. Travis CI or Jenkins CI, and adoption, positive effects have been demonstrated on pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential. Here we study the adaptation and evolution of code writing, review practices and unit testing practices as Travis CI is adopted by hundreds of established projects on GitHub. By employing a mix of quantitative studies and case studies we triangulate the general evolution trajectories, and provide reasoning for the differences encountered among the projects.
\end{abstract}

\section{Introduction}
Devops, or bringing development and build/release activities in the same framework can bring changes in the product to the user-plane more quickly. For it to be effective, the technology that implements these ideas has to allow for a seamless back and forth between development, integration, code testing review, and release. 

Continuous Integration is the part of devops that seamlessly builds, tests, and integrates developer changes, and performs any pre-specified testing. CI (\eg Travis CI, Jenkins CI, Hudson CI), if implemented properly, can benefit the distributed software development process, in particular code change throughput~\cite{Stolberg} and some aspects of code quality~\cite{VasilescuYWDF15}. It is a disruptive technology, in that it can scale up distributed development without noticeable diminishment in quality.

Proper implementation is key here, otherwise the benefits may not be felt, and the technology may become a drag on resources, since it subsumes continuous builds and testing. 
This is particularly true in team environments, where it falls to each individual developer to keep up with project specific implementation policies.
For that reason, Martin Fowler wrote an article on CI best practices~\cite{}, which has been very influential and in many projects there are expectations that those practices define CI and that they will be followed.~\cite{}
In particular, those practices are: Maintain a Single Source Repository,  Automate the Build, Make Your Build Self-Testing, Everyone Commits To the Mainline Every Day, Every Commit Should Build the Mainline on an Integration Machine, Fix Broken Builds Immediately, Keep the Build Fast, Test in a Clone of the Production Environment, Make it Easy for Anyone to Get the Latest Executable, Everyone can see what's happening, and Automate Deployment.
But to what extent are they followed in practice?

In this paper we sought to examine the extent to which best practices of CI are actually transitioned to and followed, after CI adoption in online software development projects.  We focus on three major aspects of modern development: practices related to code changes, code testing, and code reviews, and operationalize them into measures for them which we observe from data of GitHub repositories.
We introduce regression discontinuity desing analyses in order to evaluate the effect of an intervention, in our case CI adoption, on the transition toward expected behaviors in the above three practices.
Applying those methods to hundreds of projects, appropriately selected, we find that:

\begin{itemize}

\item There is a significant and persistent downward trend in commit churn after CI is adopted, over all projects, but also for a large fraction of individual projects. This trend is non-existent before the adoption.

\item Consistent with the above, we also find that the number of commits and PRs increases after CI adoption, and is random before it.

\item There is a rapidly increasing trend in the number of automatic test per PR after CI adoption.

\item There are significantly more issues submitted after CI adopted than before.

\item There are no significant changes in the frequency of commits after CI adoption.
\end{itemize}


In what follows, we tell this paper's story in a carefully crafted, yet mostly standard multisectioned format, beginning with the inimitable background and theory section, followed by an examplary part on methodology, culminating in our tight results and logical discussion, with our long reaching conclusions, and the unavoidable threats to validity section bringin up the rear.


\section{Background and Theory}

For a developer not proficient with the operations side of the process, transitioning to an integrated CI platform, like Travis CI, involves adaptation of their established processes to the new environment. During this transition, some developers will experience a more streamlined process evolution trajectory than others. Studying those trajectories can provide lessons learned.


We expect the following to potentially change in the transition <need to write theory behind each>:

On the developer side:
Change in code writing/committing: we expect smaller change sets over time
We expect More unit testing over time
Operations side:
More discussions in code review over time
Different categories of initial faults

Continuous integration encourages developers to ``break down their work into small chunks of a few hours each'', as smaller and more frequent commits keep them to keep track of their progress and reduces the debug effort~\cite{Fowler,Duvall}. %Duvall [p. 31,38,40]
Therefore, in \textbf{RQ1} we investigate whether introduction of the continuous integration has indeed led to smaller commits.
\as{Why do not we look at their frequency? In an early study Miller has observed that on average Microsoft developers committed once a day, while off-shore developers committed less frequently due to network latencies~\cite{Miller}; Y\"{u}ksel reports 33 commits per day~\cite{Yuksel}.}

RQ1: Are developers reducing the size of code changes in each commit post CI adoption? Do they continue to do so over time?

Moreover, continuous integration is closely related to presence of automated tests~\cite{Fowler}. Duvall even claims that continuous integration without such tests should not be considered continuous integration at all~\cite{Duvall}, while Cannizzo, Clutton and Ramesh deem an extensive suite of unit and acceptance tests to be an essential first step~\cite{CannizzoCluttonRamesh}. \as{Y\"{u}ksel reports increase of the number of automated tests but they have combined introduction of CI with automated testing~\cite{Yuksel}. }

RQ2: How are developers transitioning to automated testing over time?

For continuos integration to have the stated benefits, code review should play a prominent role. In a pull-request model of development, code review is done through comments in open issues.

RQ3: Are developers transitioning to using more issues after the adoption of CI?


%\subsection{RQs}

\input{method}

\section{Results and Discussion}

We sought to study how different software development practices evolve around the time of CI adoption and the period after the adoption.

\subsection{RQ1: Changes in Code Churn Practices}

The first development practice we examine is code churn.
The data consists of 567 projects, each with at least 500 nonmerge commits.

\noindent \underline{Exploratory Study} To explore general trends over time, we first look at code churn in months leading up to CI adoption.
Fig.~\ref{Fig:CodeChurnBefore} shows a boxplot of per-project median code churn for each of nine consecutive 30-day time intervals before CI adoption.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.
We observe that for most part, the medians dance around 
10 and 12 lines of code churn per period, and the averages between 16 and 18 lines, with large variance and no obvious trend. The 30-day interval just before CI adoption seems to stand out and is higher than the rest.

Next, we look to the other side of the adoption point. Fig.~\ref{Fig:CodeChurnBefore} shows a boxplot of per-project median code churn for each of nine consecutive 30-day time intervals following CI adoption.
Here, the story that the boxplots are telling is different. There is an apparent, downward trend in both the medians and averages, the former drop monotonically from 13 down to 7-8 lines of code and the latter down to 15 lines of code.
The trend in the medians is significant statistically (which test?, p-val = 0.0014).
The 30- and 60-day intervals just after the adoption point seem to be higher than the rest, and together with the 30-day interval just before adoption of CI seem to form a peak of increased code churn.
In conclusion, discounting the peak behavior in code churn around CI adoption time, we observe a 30\% reduction in the amounts of code churn from before to after CI adoption.


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{churn_before.pdf}
\caption{The Code churn before CI adoption}
\label{Fig:CodeChurnBefore}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{churn_after.pdf}
\caption{The Code churn after CI adoption}
\label{Fig:CodeChurnAfter}
\end{figure}

\noindent \underline{Modeling Study} 
Guided by our observations from the exploratory study above, we proceed to quantify the trends we observed.
For each project, we fitted a sharp RDD model, as decribed before.
Table~\ref{Table:rddmodels} summarizes the results.

Table XX: Summary of all models

Table XX: Projects with statistical negative trends after CI

\noindent \underline{Discussion}
The dominant trend following CI adoption is a downward one, twice as frequent as the upward one.
This is consistent with Fowler's good practices of CI, of committing more frequently and smaller pieces of code.
Still, most projects end up not following a trend after their adoption of CI, while still remaining active.


The increased code churn on both sides near the CI adoption time is arguably in line with expectations that more maintanance work may be going on in preparation for the transition to CI, and that the projects go through some adjustment/cleanup period right after.

\noindent \underline{Case Study: Transition Period}

\subsection{RQ2: Trends in Testing}

The second development practice we examine is testing and the evolution of the types of errors revealed by automated testing.
The data consists of XXX projects, each with at least YYY tests.

\noindent \underline{Exploratory Study} As before, we first look for general trends over time.
Fig.~\ref{Fig:Tests} shows a boxplot of per-project median number of tests per build, for each of five consecutive 60-day time intervals before CI adoption.
We aggregated the data here in 60 day intervals to make it easier to visualize the trend since the differences are small.
The vertical line in each boxplot is the median value of all per-project median, and the black dot is their average value.
We observe a monotonically increasing trend in both the medians, from 140 to 160 tests per build, and the means, from 205 to 245 tests per build, i.e. 15\% to 20\% increase. The trend in the medians is significant statistically (which test?, p-val = 0.00?).


\noindent \underline{Discussion}

\subsection{RQ3: Changes in Issues}

Fig 5: Box plots of \# issues per unit time period, one for each time point, each point an aggregate of all projects


\input{examples}
\input{rw}


\section{Conclusion}

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{issues_before.pdf}
\caption{Median number of issues before CI adoption}
\label{Fig:IssuesBefore}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{issues_after.pdf}
\caption{Median number of issues after CI adoption}
\label{Fig:IssuesAfter}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{tests.pdf}
\caption{Unit tests per build following CI adoption}
\label{Fig:Tests}
\end{figure}

\section{Threats to Validity}

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{plot_together.png}
\caption{Evolution of Error Types Since CI Adoption}
\label{Fig:BugTypes}
\end{figure}


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}