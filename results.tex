% !TEX root = CI_Adoption.tex

\section{Results and Discussion}
\label{sec:results}
We discuss changes in development practices after CI adoption along four 
dimensions: commit frequency, code churn, pull requests resolution efficiency,
issue tracking, and testing. 

%\subsection{Preliminaries}
%\label{sec:examples}
%
%Before diving into quantitative analysis we eyeballed several projects in our 
%collection. 
%We observed that on several occasions \Tvis has been adopted as part of a 
%larger scale restructuring effort. 
%For instance, \texttt{allendevco/pill-logger} has started using \Tvis on November 
%20, 2013; in the same period developers have changed the project build system 
%from Ant to Gradle and, as a consequence of that, also changed the way the 
%project folders are organized.
%Similarly, \texttt{Netflix/denominator} has introduced \Tvis on March 13, 2015
%after an active development period of daily commits starting on March 3.
%On the very same date \Tvis has been introduced, the project updated library 
%dependencies and restructured tests, while on the next day it added examples 
%and removed support of DiscoveryDNS.
%This rather active development continued until the version 4.5.0 has been 
%released on April 6, 2015.
%The project had three additional commits in April and then went to a recess 
%until early August. 
%Examples such as \texttt{allendevco/pill-logger} and \texttt{Netflix/denominator} 
%suggest that the activity immediately prior and immediately after introduction 
%of \Tvis might not be representative for the project's overall software development
%practices.
%Therefore, to limit the risk of our trends being biased by extraordinary project 
%activity during the transition period, we decided to exclude one month before 
%the first \Tvis build and one month after it, in our quantitative analyses below.




\subsection{RQ1: Trends in Commit Frequency}

%Starting from RQ1 and RQ2 the practice being investigate should be �Commits To the Mainline Every Day�, but then you analyze git commits. Not surprisingly, commits do not increase. Why they should? The main reason of having many commits is the ability to partition a large change in many smaller ones and to better handle the work locally. What really matters for the CI are pushes and pull/requests. Indeed, I would expect an increase of pushes/pull requests when CI is being adopted. Maybe a way to observe this is to actually analyze merge commits and see whether their frequency increases.
%Similar considerations apply for the size of code changes. Also, I�ve the feeling that such a size may strongly depend on the kind of activity being performed. For example, it could be that bug fixing correlates with small change size, whereas feature addition with larger ones. Unfortunately, such a factor is not controlled.


The first practice we examine is commit frequency.
As we investigate the ``Commits to the Mainline Every Day'' practice, it is
important to distinguish non-merge from merge commits.
Indeed, local git commits, in a developer's offline repository, happen in 
isolation and can be seen as simply a mechanism to partition the work.
However, what matters for \Tvis are pushes and pull requests to the blessed 
\GH repository, \ie only then \Tvi would be triggered.
Push events are not readily accessible in our data, hence we analyze 
merge commits on the main development branch as a proxy for work
that would have been subjected to CI by \Tvi.
Since not all our projects have recorded merge commits in \emph{all} 24 
periods, we restrict this analysis to only 575 projects that had at least one 
merge commit in each time window.

%(we obtained the latter as described previously in the Methods section).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 32]{figures/freq-non-merge1.pdf}
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/freq-merge1.pdf}
\caption{Commit frequency before and after the \Tvis adoption.}
\label{fig:freq}
\end{figure}


%We started from the same data as before, 567 projects, each with at least 500 
%non-merge commits.
%Additionally, we restricted the projects for the  merge commits study so that we only look at projects that have at least 20 merge commits over the 18 time points, \ie on average about 1 per time point.
%This filtering yielded a set of 370 projects for the merge commit frequency study.

%\begin{figure}[!t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{numbercommits.pdf}
%\caption{Non-merge commit frequency before and after CI adoption}
%\label{Fig:NumberCommits}
%\end{figure}
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=0.45\textwidth, height=1.5in, clip=true, trim=0 15 15 50]{merges.pdf}
%\caption{Merge commit frequency before and after CI adoption}
%\label{Fig:MergeCommits}
%\end{figure}

\smallskip\noindent \emph{Exploratory Study}: 
Figure~\ref{fig:freq} shows the boxplots of per-project number of non-merge 
commits (top) and number of merge commits (bottom), respectively, for each
of twelve consecutive 30-day time intervals before and after the \Tvis adoption.
Note the log scale.
Recall that due to instability, we omit the 30-day time interval centered 
around $t = 0$, when the earliest commit to the \Tvi configuration file, which 
signals adoption, was recorded (Figure~\ref{fig:timeseries}).
The horizontal line in each boxplot is the median value across all projects.

First, we observe relative stability in the number of non-merge commits prior 
to the \Tvis adoption (Figure~\ref{fig:freq}, top), with the across-projects medians 
around 78 commits, and a slight decreasing trend after the adoption, with the
across-projects median dropping to 67 commits in period 12.
Second, after an initial dip in periods -12 to -10, the merge commits 
(Figure~\ref{fig:freq}, bottom) appear to display a slight increasing trend prior 
to the \Tvis adoption, with the across-projects median reaching 18 commits 
immediately prior to the adoption period, followed by apparent stabilization 
after that.
We also observe a discontinuity at $t=0$: the across-projects median is 21 
commits right after the adoption period.
Note, in both plots, the large variance in the data.

%To explore general trends over time, we first look at commit frequency in 
%the months leading up to CI adoption.
%Figs.~\ref{Fig:NumberCommits} and \ref{Fig:MergeCommits} show the boxplots 
%of per-project median number of non-merge commits, and merge commits, 
%respectively, for each of nine consecutive 30-day time intervals before CI adoption 
%and, respectively, nine consecutive 30-day time intervals after CI adoption.
%We use a log-scale for the merge commits due to the overdispersion of the data 
%which is sparse over the time periods we have chosen.
%As before, the horizontal line in each boxplot is the median value of all 
%per-project medians, and the black dot is their average value.

%As with code churn, here we also see a peak in the 30-day 
%interval just before and the one just after CI adoption.
%We do not observe an apparent trend in the 
%median or average non-merge commit frequency.
%We do observe an upward trend in merge commits, but the variance in the data is large.

\smallskip\noindent \emph{Statistical Modeling Study}: 
We fitted a mixed-effects RDD model, as described before, to model trends
in the \emph{number of merge commits} per project, over time, as a function 
of \Tvis adoption, and while controlling for confounds; most notably, we control 
for the \emph{number of non-merge commits}, as these may have not all been 
subjected to CI, since they appear to display a decreasing trend with time.

\input{tables/freq-model2}
%\input{tables/freq-model}

Recall that we modeled a random intercept for \emph{programming language}, 
to allow for language-to-language variability in the response (\ie code in some 
languages being naturally more verbose than in others, resulting in different
ways to split the work across commits; or community-level norms for committing), 
and a random \emph{intervention} slope and intercept for \emph{project}, to allow 
for project-to-project variability in the response and the possibility that, on average, 
projects with lower initial activity may be less strongly affected by adopting 
\Tvis than high-frequency projects.
Recall also that the coefficient for \emph{time} is the slope before adoption,
the coefficient for \emph{intervention} is the size of the effect of CI introduction,
the coefficient for \emph{time after intervention} is the divergence in the slopes
before and after \Tvis adoption, and the sum of the coefficients for \emph{time}
and \emph{time after intervention} is the slope of the linear trend after \Tvis
adoption.

Table~\ref{table:freq} summarizes the results.
In addition to the model coefficients and corresponding standard errors, the table 
shows the sum of squares, a measure of variance explained, for each variable.
The statistical significance is indicated by stars.
The fixed-effects part of the model fits the data well ($R_m^2=0.58$).
There is also a considerable amount of variability explained by the random
effects, \ie project-to-project and language-to-language differences, not 
explicitly modeled by our fixed effects ($R_c^2=0.83$).
Among the fixed effects, we observe that the \emph{number of non-merge 
commits}, our main control, explains most of the variability in the response.
The coefficient is positive, \ie the direction of the correlation is expected: 
other things constant, the more non-merge commits there are, the more 
merge commits there will be as well.
Neither \emph{project size} (total number of commits over the entire history) nor 
\emph{project age} at adoption time have any statistically significant effects.

Next we turn to our \Tvi-related variables.
The model confirms a statistically significant, positive, baseline trend in the response 
with \emph{time} (with a small effect), as suggested by our exploratory study.
The model does not detect any discontinuity at adoption time, since the coefficient 
for \emph{intervention} is not statistically significant.
Post adoption, there is a decrease in the slope of the time trend, but the overall 
trend remains ascending (the sum of the coefficients for \emph{time} and 
\emph{time after intervention} is positive): more merge-commits as time passes.

%As before, we eliminated one time point on either side of the CI adoption 
%time, so that the peaks at those times  do not throw off the regression models.
%\input{tables/rq2_models}

%For the non-merge commits, in 452 (80\%) of the models, there was no significant linear relationship 
%between  frequency and time, \ie there was no linear trend.
%In 71 projects there was a rising trend in commit frequency before CI 
%and in 44 projects a falling trend before CI.
%Similarly, in 466 projects no effect of CI introduction on commit frequency 
%was found. 
%Among the 101 projects where an effect was significant, in 59 the trend 
%was positive, \ie commit frequency increased, and in the rest it was 
%negative, \ie commit frequency decreased.
%The mean value over all projects for $\gamma$, the effect size of CI 
%adoption, was $1.1$ commit.
%While in 454 projects no change of slope in the trends before and after CI 
%adoption was found, in the 113 with significant change of trends a reduction 
%in the slope was more prevalent than an increase by about 2.5 to 1.
%This carried over to a lesser degree into trends in commit frequency 
%following CI adoption: out of the 113 significant trend changes, 71 are 
%downward trends, and 42 are upward.
%
%For merge commits (Table~\ref{Table:rddmodels_merge_freq}), the results 
%are qualitatively similar to those for non-merge commits.

\smallskip\noindent \emph{Discussion}:
The exploratory study suggests a slight decreasing trend in the number 
of non-merge commits with time.
Separately modeling the trend in \emph{number of non-merge commits}
(not shown) using a similar approach as above confirmed this overall 
decreasing trend.
This is consistent with the common observation that, as projects move 
past the initial development hurdle and age, development generally slows 
down, and the focus shifts to bug-fixing rather than adding new features, 
noted, \eg by Brindescu \etal~\cite{brindescu2014centralized}.

In contrast, the statistical modeling study above revealed a consistent 
increase in \emph{merge commits} with time, including post-CI adoption, 
albeit with a slowdown.
This overall trend aligns with the expected increase in commit frequency 
after switching to CI, as expressed and encouraged by Fowler.
Given the overall decreasing trend with time in the number of non-merge
commits, the increase in merge commits is noteworthy.
One explanation for this is that projects are switching to more distributed
development workflows, using branches and pull requests. 
Indeed, in our survey R38 has indicated that their project ``shifted towards 
pull-request - merge development strategy $<$as i$>$t made the distributed 
development more manageable.''

The development process change reported by R38 is not exceptional. 
Indeed, the idea of a shift towards more focused development in separated 
branches has been voiced by R32 (``shorter lived (and generally single contributor) 
branches'') and R49 (``feature branches''), R6, and R47.
% both report more 
%emphasis on branches.
%R6 speaks of ``much more emphasis'' being put on using 

%\as{R38 The release process got changed. Additionally we have shifted towards pull-request - merge development strategy. It made the distributed development more manageable.}
%\as{R44 develop branch is more stable; we can trust it more and release sooner}
%\as{R46 Regular releases \& work on development branch}
%\as{R54 We've moved as much as possible into CI (eg doc generation, building release artifacts, etc.).}

%As was the case for code churn, most projects did not have longitudinal 
%linear trends in non-merge commits, for reasons probably similar to those 
%in case of code churn.

%We observed a post-CI adoption increase in merge commits 

%in our exploratory study, the statistical tests did not confirm that to be the 
%case for more than 20\% of the projects. 
%We attribute this disparity to the large variance in the data and especially 
%the sparsity of merge commits over time. 
%
%While our exploratory study did not suggest presence of a trend, the 
%modeling study suggests that right after CI adoption there is a slight 
%positive effect in both the merge and non-merge commits (after removing the peak months around CI adoption time). 
%However, this effect is not sustained.
%% if a linear trend is present, which is 2.5 times as likely to be decreasing as increasing. 

%Given the expected increase in the frequency of commits expressed and 
%encouraged by Fowler, our results might appear surprising.
%It is possible that some saturation to the commit frequency has already 
%been reached even before the CI adoption time.
%As we see next, it is also likely that instead developers are spending more time on 
%automated testing.




\subsection{RQ2: Trends in Code Churn}

The next development practice we examine is code churn.
Recall that our data consists of \bv{2,401 projects across 22 programming languages.}
As before, we distinguish between merge and non-merge commits.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/churn-non-merge.pdf}
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/churn-merge.pdf}
\caption{Mean code churn per commit per project, before and after \Tvi.}
\label{fig:churn}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}: 
Figure~\ref{fig:churn} shows boxplots of per-project per-commit mean code 
churn (medians behave similarly) for each of twelve consecutive 30-day time 
intervals before, and after, the \Tvis adoption.
The horizontal line in each boxplot is the median value over all projects.

We observe that, in the non-merge commits, the medians are quite stable
across the entire interval, from before to after the \Tvis adoption, dancing 
around 100 
% pre-\Tvi, the medians are quite stable, dancing around 65 
lines of code churn per commit on average, with large variance.
%The 30-day interval just before \Tvis adoption (-1) seems to stand out and is 
%slightly higher than the rest (median = 73 LOC), as is the 30-day interval just after 
%\Tvi (1).
In comparison, in the merge-commits, we observe more than usual variance
in the two months preceding the adoption, as well as a slight downward trend 
%On the other side, following \Tvis adoption, 
in the medians following adoption, which drop to about 263 lines of code churn 
per commit on average in the last period (12), compared to about 405 right after 
adoption (period 1).
However, we again note the large variance around the medians in all periods.

In conclusion, non-merge commits seem mostly unaffected by time passing and 
the \Tvis adoption, while merge commits seem to be getting smaller with time.
%discounting the peak behavior in code churn around adoption time, 
%we observe a small reduction in the amounts of code churn from before 
%to after the adoption.

%To explore general trends over 
%time, we first look at code churn in months leading up to CI adoption.
%The horizontal line in each boxplot is the median value of all per-project medians, 
%and the black dot is their average value.
%We observe that, for most part, the medians dance around 10 and 12 lines of 
%code churn per period, and the averages between 16 and 18 lines, with large 
%variance and no obvious trend. 
%The 30-day interval just before CI adoption seems to stand out and is higher 
%than the rest.

%Next, we look to the other side of the adoption point. 
%Fig.~\ref{Fig:CodeChurnAfter} shows a boxplot of per-project median code 
%churn for each of nine consecutive 30-day time intervals following CI adoption.
%Here, there seems to be a slight downward trend in both the medians and 
%averages; the former drop monotonically from 13 down to 8 lines of code and 
%the latter down to 15 lines of code.
%However, we note the large variance around the medians.

%The 30- and 60-day intervals just after the adoption point seem to be higher 
%than the rest, and together with the 30-day interval just before adoption of CI 
%seem to form a peak of increased code churn.
%In conclusion, discounting the peak behavior in code churn around CI adoption 
%time, we observe a small reduction in the amounts of code churn from before 
%to after CI adoption.

%\input{tables/rq1_models}
\input{tables/churn-model}
\input{tables/churn-model2}

\smallskip\noindent \emph{Statistical Modeling Study}: 
Guided by our exploratory observations above, we proceed to quantify the 
trends we observed using a mixed-effects RDD model, as described before.
%Recall that we modeled a random intercept for \emph{programming language}, 
%to allow for language-to-language variability in the response (\ie code in some 
%languages being naturally more verbose than in others), and a random 
%\emph{intervention} slope and intercept for \emph{project}, to allow for 
%project-to-project variability in the response, and the possibility that, on average, 
%projects with lower initial churn values may be less strongly affected by adopting 
%\Tvis, since their commits are already ``small enough'' for efficient CI.
%Recall also that the coefficient for \emph{time} is the slope before adoption,
%the coefficient for \emph{intervention} is the size of the effect of CI introduction,
%the coefficient for \emph{time\_after\_intervention} is the divergence in the slopes
%before and after \Tvis adoption, and the sum of the coefficients for \emph{time}
%and \emph{time\_after\_intervention} is the slope of the linear trend after \Tvis
%adoption.

Table~\ref{table:churn} summarizes the model.
In addition to the model coefficients and corresponding standard errors, the table 
shows the sum of squares, a measure of variance explained, for each variable.
The statistical significance is indicated by stars.
First, we observe that only the combined fixed-and-random effects model fits the 
data well ($R_c^2 = 0.46$ compared to $R_m^2 = 0.1$), \ie most of the explained 
variability in the data is attributed to project-to-project and language-to-language
variability, rather than any of the fixed effects, either controls or RDD-related.

Next, we turn to our controls and main predictors.
The controls behave unsurprisingly.
Overall bigger projects (\emph{TotalCommits}) tend to churn more, other things
held constant; older projects (\emph{AgeAtTravis}) tend to churn less, perhaps 
as they have reached maturity and a more stationary stage in their evolution;
projects with a larger contributor base (\emph{NumAuthors}) also tend to churn
less, which is in line with the expectation that occasional contributors to open 
source projects are generally less active than core developers.

After controlling for confounds, we move on to the main predictors. 
We observe that the coefficient for \emph{time} is not statistically significant, 
\ie there is no baseline trend in commit churn before CI adoption, and the 
\emph{intervention}, although statistically significant, has a small effect.
However, \emph{time\_after\_intervention} has a statistically significant, 
negative coefficient, with a notable effect size.
Taken together, this confirms a decreasing trend in commit churn after 
adopting \Tvis.

\smallskip\noindent \emph{Discussion}:
Across our large sample (2,401 projects across 22 programming languages),
our modeling study revealed a statistically significant, but small, discontinuity
in the commit churn time series when adopting \Tvis; and a statistically 
significant decreasing linear trend with time in code churn, after adopting \Tvis.
The former is not unexpected, as one can reasonably expect that more 
maintenance work may be going on in preparation for the transition to \Tvis, 
and that the projects go through some adjustment/cleanup period right after.
The increased code churn on both sides near the CI adoption time is also
indication of this, as are the survey results.  
Indeed, when asked about the introduction of \Tvis, respondents frequently refer to 
test automation being introduced around the same time as \Tvi ``as contributors 
couldn't be trusted to run test suite on their own'' (R25). Furthermore, respondents 
indicate that \Tvi has been introduced as ``a part of automated package/release effort'' (R38) 
and to ``deploy artifacts to S3 on each commit, as part of our deployment process using 
Amazon CodeDeploy'' (R34).

The latter is consistent with Fowler's recommended good practices of CI, of 
committing smaller changes to the code.
The expected decrease in the size of the commits is echoed by one of the 
survey respondents: ``commits became smaller and more frequent, to check 
the build; pull requests became easier to check'' (R4).
However, the decreasing trend we observed is not steep.
Survey responses provide a possible explanation: several developers 
referred to pull request integration as the reason for introducing \Tvis, and 
while R31 and R50 state that \Tvis is used both for commits and for pull 
requests, R11, R21, and R37 explicitly state that \Tvis is applied solely to 
pull requests, \ie in those projects push commits are outside the scope of \Tvis.
In other words, if in a project not all commits are subjected to \Tvis, then 
there may be less incentive to follow Fowler's recommended good practices 
related to commit churn.

At the same time, a different respondent indicated that \Tvis discourages 
him/her from making trivial commits (R11), suggesting instead that the 
commits he/she makes are likely to be larger;
therefore, our global decreasing trend may be weakened by local trends
in the opposite direction.
Our model found evidence for strong project-specific effects: 
project-to-project and language-to-language differences, as captured by 
our random effects, better explain the overall data variability. 
This suggests that any phenomena giving rise to pressures to increase 
or reduce code churn are perhaps subordinate in magnitude to other, 
more pressing phenomena of local, \ie project-specific, character.

Alternatively, the decreasing trend in commit churn is also consistent with 
the observation by Brindescu \etal~\cite{brindescu2014centralized} that, 
as projects age, bug-fixing commits, which on average are smaller, 
become more prevalent than commits adding new features, which on 
average are larger.

\as{Say something about SE practice/research implications}
Our results suggest that while the Fowler's good practice of small commits is followed to some extent but the project-to-project and language-to-language differences are more important and might overshadow the overall trend.
\as{Still thinking} 


%Most of the projects did not show linear trends with time in code churn, on 
%either side of the CI adoption time.
%One explanation consistent with this is that any phenomena giving rise to 
%pressures to increase or reduce code churn are not long term/sustained or, 
%alternatively, perhaps they are subordinate in magnitude to other, more 
%pressing phenomena of local character in time.
%It is also likely that our models and data are insufficient to capture them.
%Some of the reasons for this are: low temporal resolution, high dispersion 
%of the data, non-uniform distribution of the events across time, and project 
%specific considerations that we could not model.

%Our exploratory study showed a peak around CI adoption time and a slight 
%downward trend following CI adoption.
%The modeling study showed that when linear trends were present (in 15\% 
%of the models), the downward one after CI adoption were twice as frequent 
%as the upward one, a reversal of the pattern in the trends from before CI.
%This is consistent with Fowler's recommended good practices of CI, of 
%committing smaller changes to the code.
%Alternatively, it is also consistent with the observation by Brindescu 
%\etal~\cite{brindescu2014centralized} that, as projects age, bug-fixing 
%commits, which on average are smaller, become more prevalent than 
%commits adding new features, which on average are larger.

%The models also showed that when present, the effect of CI adoption on the 
%amount of code churn right after was positive in more cases than negative.
%The increased code churn on both sides near the CI adoption time is 
%arguably in line with expectations that more maintenance work may be 
%going on in preparation for the transition to CI, and that the projects go 
%through some adjustment/cleanup period right after.



%The model fits the data well; it explains 35% of the variability in the data using only the fixed effects (Rm2 = 0.35), and 55% when considering both the fixed and random effects (Rc2 = 0.55).

%deviations in intercept and slope of that subject�s time trend from the population values.
%(1 + Days | Subject)
%we allow for the possibility that, for example, subjects with higher initial reaction times may, on average, be more strongly affected by sleep deprivation.

%For each project, we fitted a sharp RDD model, as described before.
%Recall that because of the peak we observed, we eliminated the time point 
%immediately before and the one immediately after the CI adoption time, so as 
%they do not throw off the regression models.
%Table~\ref{Table:rddmodels} summarizes the results for the models on the 
%remaining 16 time points.
%Recall that $\beta$ is the slope before, $\gamma$ the size of the effect of 
%CI introduction, $\delta$ is the divergence in the slopes before and after CI 
%adoption, and $\beta + \delta$ the slope of the linear trend after CI adoption.

%Of the 567 models, in 487 there was no significant linear relationship of 
%code churn to time, \ie there was no linear trend.
%In 47 projects there was a rising trend before CI and in 33 projects a falling 
%trend before CI.
%Similarly, in 507 projects no effect of CI introduction on code churn was found. 
%Among the 58 projects where an effect was significant, in 34 the trend was 
%positive, \ie code churn increased, and in 24 it was negative, \ie code churn 
%decreased.
%Interestingly, while in 487 projects no change of slope in the trends before 
%and after CI adoption was found, in the 80 projects with significant change 
%of trends, a reduction in the slope was more prevalent than an increase by 
%about 2 to 1.
%This carried over into trends following CI adoption: out of the 80 significant 
%trend changes, 55 are downward trends, and only 25 are upward.


%Table XX: Projects with statistical negative trends after CI


\bv{I'm here}


%\begin{figure}[t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{churn_before.pdf}
%\caption{Code churn before \Tvis adoption.}
%\label{Fig:CodeChurnBefore}\vspace{-0.3cm}
%\end{figure}
%
%
%\begin{figure}[t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{churn_after.pdf}
%\caption{Code churn after \Tvis adoption.}
%\label{Fig:CodeChurnAfter}\vspace{-0.3cm}
%\end{figure}



%\as{Vladimir, do you have a cross-combination data, such as \# decrease in churn AND increase in frequency?}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/issues.pdf}
\caption{Evolution of the number of closed issues before and after the \Tvis adoption.}
\label{fig:issues}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/prs-full.pdf}
\caption{Evolution of the number of closed pull requests before and after the \Tvis adoption.}
\label{fig:prs}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/pr-latency-mean.pdf}
\caption{Evolution of the mean PR latency before and after the \Tvis adoption.}
\label{fig:pr-latency}
\end{figure}


\input{tables/issues-model}
\input{tables/latency-model}

\subsection{RQ3: Trends in Pull Request Closing}
The next practice we consider is pull request latency, \ie time required to close a pull request. 
Since \Tvis is invoked and runs automatically every time a pull request is submitted we expect that the developers are rapidly notified on whether their pull request passes the \Tvi-quality gate.
This would mean that they can rapidly react upon the feedback provided by \Tvi and correct the pull request source code if necessary.
Overall we expect therefore that \Tvis should shorten the pull request latency and, under the assumption of the constant productivity, increase the number of 
pull requests being closed.
\as{some literature here. Bogdan, is ``Wait for it'' relevant here?}

\smallskip\noindent \emph{Exploratory Study}:
Figures~\ref{fig:issues} and \ref{fig:prs}  shows boxplots of per-project number of closed issues and pull requests for each of twelve consecutive 30-day time 
intervals before, and after, the \Tvis adoption. 
Similarly to Figure~\ref{fig:churn} in both cases we see large variance around the medians. 
Inspecting the figures we observe the median number of closed issues/pull requests per months seems to fluctuate around 12 \as{can we take a look at the numbers?}
and while the number of closed issues does not seem to exhibit any trend, the number of closed pull requests seems to be increasing after the introduction of \Tvis.
Figure~\ref{fig:prs} provides a complementary perspective on the efficiency of pull request resolution, namely the pull request latency.
The median latency seems to increase prior to introduction of \Tvis and more or less to stabilize after \Tvis has been introduced.

\smallskip\noindent \emph{Statistical Modeling Study}:
We apply RDD in the same way we have using this approach to address \textbf{RQ1} and \textbf{RQ2}. 
Statistical models for the number of issues closed, number pull requests closed and the mean pull request latency are summarized in Tables~\ref{table:issues}, \ref{???} and \ref{table:latency}.
As above, the combined fixed-and-random effects models fit the data much better than the basic fixed-effect models indicating that the project-to-project and language-to-language variability 
are responsible for most of the variability explained. 
$R^2_m$ and $R^2_c$ are in line with the statistical models built for \textbf{RQ2} (churn) with $R^2_m \simeq 0.1-0.2$ and $R^2_c \simeq 0.4-0.5$.

Taking a closer look at Table~\ref{table:latency} we observe that the number of authors
is statistically significant and explains a large part of the variance explained by the model.
Recall, that as opposed to Yu et al.~\cite{yue2015wait} we consider as authors \emph{both} core developers with the commit rights and external contributors that can only submit pull requests. 
Our model indicates that projects with larger number of authors tend to have a higher pull
request latency, suggesting that the larger number of authors is achieved through a high
number of external contributors submitting pull requests and therefore slowing down the pull request review process as each pull request requires an extra review/merge effort from a limited number of core developers.
Variable \emph{time} is also statistically significant and explains a big percentage of the variance. 
This means that there is a base-line increase in the pull request latency; introduction of \Tvis does not result in a discontinuity (\emph{intervention} is not statistically significant) but merely slows down the increasing trend.



\smallskip\noindent \emph{Discussion}:

\subsection{RQ4: Trends in Issue Report Frequency}

The 3rd practice we examine is issue report frequency.
As above, we use data on 293 projects, each with 
100+ issues.

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{issues_before.pdf}
\caption{Median number of issues before CI adoption}
\label{Fig:IssuesBefore}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{issues_after.pdf}
\caption{Median number of issues after CI adoption}
\label{Fig:IssuesAfter}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}:
We follow the same approach as in RQ1 and RQ2 and compare the 
medians and the averages in the number of issues per time period, in the 
months before and after CI adoption.
Fig.~\ref{Fig:IssuesBefore} and \ref{Fig:IssuesAfter} plot the number of 
issues per unit time period.
Observing these figures shows that the months immediately preceding 
and following the adoption of CI exhibit the highest number of issues.
Besides those, before CI adoption the median number of issues per period seem to vary between 2 and 4, with most of them being at 3, while the averages are between 5 and 6.
Following CI adoption the median of the number of issues seems to 
stabilize at 4 per period, and the averages are between 6 and 7.

\input{tables/rq3_models}

\smallskip\noindent \emph{Statistical Modeling Study}:
Results of the RDD modeling are summarized in Table~\ref{Table:rddmodels_issues}.
The time period before and the one after the time of CI adoption have been removed from the modeling, as explained in RQ1 above.
The lower number of projects in Table~\ref{Table:rddmodels_issues} 
compared to Tables~\ref{Table:rddmodels} and~\ref{Table:rddmodels_freq} 
should not be surprising. 
Recall from Section~\ref{sec:dd} that to answer RQ3 we have focused on 
projects having at least 100 issues, further reducing the size of the data set 
used in RQ1 and RQ2 to 293.
We have fitted models to 291 projects of the 293, however in most cases we 
did not observe linear trends.
When a linear trend has been observed than it was more than twice more 
often decreasing than increasing.
The mean value over all projects for $\gamma$, the effect size of CI adoption, 
was $1.0$ issue.


\smallskip\noindent \emph{Discussion}:
Again, most of the projects did not have longitudinal linear trends for reasons probably similar to those in the case of the code churn.

As in RQ1, our exploratory study showed a peak in the median number of issues per period around CI adoption time, and an upward trend following CI adoption.
The modeling study showed that when linear trends were present (in 20\% 
of the models), the downward one after CI adoption were 2.5 times as frequent 
as the upward one, a reversal of the pattern in the trends from before CI.
This has to be considered in combination with the size of the effect of CI adoption, which is $1$ issue per period additional.
Thus, the number of issues increases significantly overall after CI adoption, to the tune of $33\%$, and then has a pressure of slight reduction over time.

This is consistent with an increased number of issues brought up after CI adoption. Some of those are invariably related to the change in the way things are done, and the transition. Others may be related to the increased automated testing which likely discovers more concerns and bugs, which are then mentioned in issues.
Overall, this result shows that the adoption of CI brings about more issues, as expected.

\as{Survey}
Survey participants experience \Tvi as beneficial for bug detection: ``I think we produce less bugs'' (R45), ``there was an immediately noticeable improvement in terms of the number of serious bugs in production'' (R51). 
An interesting insight into this matter is provided by R16: while several survey respondents have indicated that \Tvis is not consistent in terms of performance and relatively slow, those performance aspects provided R16 with means to detect ``more flaky issues'' ``which where only visible on \Tvi''. 

\subsection{RQ4: Trends in Testing}

The last development practice we examine is testing and the evolution 
of the types of errors revealed by automated testing.
The data consists of 250 projects, each with at least 100 builds, and 90\% 
of builds executing tests.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{tests.pdf}
\caption{Unit tests per build following CI adoption}
\label{Fig:Tests}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}: 
As before, we first look for general trends over time.
Fig.~\ref{Fig:Tests} shows a boxplot of per-project median number of tests 
per build, for each of five consecutive 60-day time intervals before CI adoption.
We aggregated the data here in 60 day intervals to make it easier to visualize 
the trend, since the differences are small.
The horizontal line in each boxplot is the median value of all per-project medians, 
and the black dot is their average value.
We observe a monotonically increasing trend in both the medians, from 140 to 
160 tests per build, and the means, from 205 to 245 tests per build, \ie 15\% to 
20\% increase. 

To ascertain if the complexity of builds increases over time, we also calculated 
the average number of jobs per build.
The median is $1$ for all five time intervals.
These two findings suggest strongly that there is an increase in the number of 
unit tests per build over time.
This, coupled with our finding that builds are not getting more complex over time indicates that developers are likely spending more time on 
automated testing since CI adoption.
This is consistent with Fowler's ``good practices" proposal.



\smallskip\noindent \emph{Error Types Study}:
We also looked at the evolution of the error types in builds over time after CI 
adoption.
For all 250 projects above, we looked at the builds that resulted in errors, 
and deconvolved the errors into 8 types, as described above.
We did this over 3 intervals: 0-60 days, 61-180 days, and 181-300 days (\ie 
corresponding to the first, third, and fifth interval in Fig.~\ref{Fig:Tests}).
The results are shown in Fig.~\ref{Fig:BugTypes}, where on the y-axis are 
the proportion of all erroneous builds that yield that error type.
We find an apparent upward trend over time in most error type categories, 
most notably compile errors, execution errors, failed tests, and skipped 
test.\footnote{The median number of error types per build remained $2$ over 
time.}
All these are consistent with an increase in the amount of code being built 
and tested per build, as well as an increasing management of errors by 
skipping tests, likely to aid in debugging.
On the other hand, we find a decreasing trend among errors related to missing 
files/dependencies, and time-outs, both consistent with those errors being less 
of an issue over time, as developers are acculturating to the CI mediated 
processes.

Thus, overall we find an expected adjustment to the automated testing and 
error handling, with an indication that debugging complexity grows over time.

%%this is the old plots
%\begin{figure}[!t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{plot_together.png}
%\caption{Evolution of Error Types Since CI Adoption}
%\label{Fig:BugTypesold}
%\end{figure}


%this is the new plots
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{new_plot_together.png}
	\caption{Evolution of Error Types Since CI Adoption}
	\label{Fig:BugTypes}
\end{figure}

\as{Survey}
Improving software quality through test automation has been repeatedly mentioned by the survey respondents as a reason to choose for \Tvis. 
Not surprisingly, respondents also indicate that changes facilitating test automation had to be made when \Tvis was introduced. 
Moreover, by using \Tvis developers became more aware of testing-related aspects of their software development process (R32, R52), 
spend more time and effort on designing testing strategies (R5, R53) and encourage other projects to start using \Tvis (R26).
Efficiency of \Tvis for long-running tests is, however, a concern (R10).

