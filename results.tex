% !TEX root = CI_Adoption.tex

\section{Results and Discussion}
\label{sec:results}
We discuss changes in development practices after CI adoption along four 
dimensions: code churn, commit frequency, issue tracking, and testing. 

%\subsection{Preliminaries}
%\label{sec:examples}
%
%Before diving into quantitative analysis we eyeballed several projects in our 
%collection. 
%We observed that on several occasions \Tvis has been adopted as part of a 
%larger scale restructuring effort. 
%For instance, \texttt{allendevco/pill-logger} has started using \Tvis on November 
%20, 2013; in the same period developers have changed the project build system 
%from Ant to Gradle and, as a consequence of that, also changed the way the 
%project folders are organized.
%Similarly, \texttt{Netflix/denominator} has introduced \Tvis on March 13, 2015
%after an active development period of daily commits starting on March 3.
%On the very same date \Tvis has been introduced, the project updated library 
%dependencies and restructured tests, while on the next day it added examples 
%and removed support of DiscoveryDNS.
%This rather active development continued until the version 4.5.0 has been 
%released on April 6, 2015.
%The project had three additional commits in April and then went to a recess 
%until early August. 
%Examples such as \texttt{allendevco/pill-logger} and \texttt{Netflix/denominator} 
%suggest that the activity immediately prior and immediately after introduction 
%of \Tvis might not be representative for the project's overall software development
%practices.
%Therefore, to limit the risk of our trends being biased by extraordinary project 
%activity during the transition period, we decided to exclude one month before 
%the first \Tvis build and one month after it, in our quantitative analyses below.

\subsection{RQ1: Trends in Code Churn}

The first development practice we examine is code churn.
Recall that our data consists of 2,401 projects across 22 programming languages.
%each with at least 500 non-merge commits.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/churn.pdf}
\caption{Mean code churn per commit per project, before and after \Tvi.}
\label{fig:churn}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}: 
Figure~\ref{fig:churn} shows boxplots of per-project per-commit mean code 
churn (medians behave similarly) for each of twelve consecutive 30-day time 
intervals before, and after, the \Tvis adoption.
Recall that due to instability, we omit the 30-day time interval centered 
around $t = 0$, when the earliest commit to the \Tvi configuration file, which 
signals adoption, was recorded (Figure~\ref{fig:timeseries}).
The horizontal line in each boxplot is the median value over all projects.

We observe that, pre-\Tvi, the medians are quite stable, dancing around 65 
lines of code churn per commit on average, with large variance.
The 30-day interval just before \Tvis adoption (-1) seems to stand out and is 
slightly higher than the rest (median = 73 LOC), as is the 30-day interval just after 
\Tvi (1).
On the other side, following \Tvis adoption, there seems to be a slight downward 
trend in the medians, which drop to about 53 lines of code churn per commit on 
average in the last period (12).
However, we note the large variance around the medians.

In conclusion, discounting the peak behavior in code churn around adoption 
time, we observe a small reduction in the amounts of code churn from before 
to after the adoption.

%To explore general trends over 
%time, we first look at code churn in months leading up to CI adoption.
%The horizontal line in each boxplot is the median value of all per-project medians, 
%and the black dot is their average value.
%We observe that, for most part, the medians dance around 10 and 12 lines of 
%code churn per period, and the averages between 16 and 18 lines, with large 
%variance and no obvious trend. 
%The 30-day interval just before CI adoption seems to stand out and is higher 
%than the rest.

%Next, we look to the other side of the adoption point. 
%Fig.~\ref{Fig:CodeChurnAfter} shows a boxplot of per-project median code 
%churn for each of nine consecutive 30-day time intervals following CI adoption.
%Here, there seems to be a slight downward trend in both the medians and 
%averages; the former drop monotonically from 13 down to 8 lines of code and 
%the latter down to 15 lines of code.
%However, we note the large variance around the medians.

%The 30- and 60-day intervals just after the adoption point seem to be higher 
%than the rest, and together with the 30-day interval just before adoption of CI 
%seem to form a peak of increased code churn.
%In conclusion, discounting the peak behavior in code churn around CI adoption 
%time, we observe a small reduction in the amounts of code churn from before 
%to after CI adoption.

%\input{tables/rq1_models}
\input{tables/churn-model}

\smallskip\noindent \emph{Statistical Modeling Study}: 
Guided by our exploratory observations above, we proceed to quantify the 
trends we observed using a mixed-effects RDD model, as described before.
Recall that we modeled a random intercept for \emph{programming language}, 
to allow for language-to-language variability in the response (\ie code in some 
languages being naturally more verbose than in others), and a random 
\emph{intervention} slope and intercept for \emph{project}, to allow for 
project-to-project variability in the response, and the possibility that, on average, 
projects with lower initial churn values may be less strongly affected by adopting 
\Tvis, since their commits are already ``small enough'' for efficient CI.
Recall also that the coefficient for \emph{time} is the slope before adoption,
the coefficient for \emph{intervention} is the size of the effect of CI introduction,
the coefficient for \emph{time\_after\_intervention} is the divergence in the slopes
before and after \Tvis adoption, and the sum of the coefficients for \emph{time}
and \emph{time\_after\_intervention} is the slope of the linear trend after \Tvis
adoption.

Table~\ref{table:churn} summarizes the model.
In addition to the model coefficients and corresponding standard errors, the table 
shows the sum of squares, a measure of variance explained, for each variable.
The statistical significance is indicated by stars.
First, we observe that only the combined fixed-and-random effects model fits the 
data well ($R_c^2 = 0.46$ compared to $R_m^2 = 0.1$), \ie most of the explained 
variability in the data is attributed to project-to-project and language-to-language
variability, rather than any of the fixed effects, either controls or RDD-related.

Next, we turn to our controls and main predictors.
The controls behave unsurprisingly.
Overall bigger projects (\emph{TotalCommits}) tend to churn more, other things
held constant; older projects (\emph{AgeAtTravis}) tend to churn less, perhaps 
as they have reached maturity and a more stationary stage in their evolution;
projects with a larger contributor base (\emph{NumAuthors}) also tend to churn
less, which is in line with the expectation that occasional contributors to open 
source projects are generally less active than core developers.

After controlling for confounds, we move on to the main predictors. 
We observe that the coefficient for \emph{time} is not statistically significant, 
\ie there is no baseline trend in commit churn before CI adoption, and the 
\emph{intervention}, although statistically significant, has a small effect.
However, \emph{time\_after\_intervention} has a statistically significant, 
negative coefficient, with a notable effect size.
Taken together, this confirms a decreasing trend in commit churn after 
adopting \Tvis.

\smallskip\noindent \emph{Discussion}:
Across our large sample (2,401 projects across 22 programming languages),
our modeling study revealed a statistically significant, but small, discontinuity
in the commit churn time series when adopting \Tvis; and a statistically 
significant decreasing linear trend with time in code churn, after adopting \Tvis.
The former is not unexpected, as one can reasonably expect that more 
maintenance work may be going on in preparation for the transition to \Tvis, 
and that the projects go through some adjustment/cleanup period right after.
The increased code churn on both sides near the CI adoption time is also
indication of this, as are the survey results \as{ToDo}.

The latter is consistent with Fowler's recommended good practices of CI, of 
committing smaller changes to the code.
The expected decrease in the size of the commits is echoed by one of the 
survey respondents: ``commits became smaller and more frequent, to check 
the build; pull requests became easier to check'' (R4).
However, the decreasing trend we observed is not steep.
Survey responses provide a possible explanation: several developers 
referred to pull request integration as the reason for introducing \Tvis, and 
while R31 and R50 state that \Tvis is used both for commits and for pull 
requests, R11, R21, and R37 explicitly state that \Tvis is applied solely to 
pull requests, \ie in those projects push commits are outside the scope of \Tvis.
In other words, if in a project not all commits are subjected to \Tvi, then 
there may be less incentive to follow Fowler's recommended good practices 
related to commit churn.

At the same time, a different respondent indicated that \Tvis discourages 
him/her from making trivial commits (R11), suggesting instead that the 
commits he/she makes are likely to be larger\as{too speculative?};
therefore, our global decreasing trend may be weakened by local trends
in the opposite direction.
Our model found evidence for strong project-specific effects: 
project-to-project and language-to-language differences, as captured by 
our random effects, better explain the overall data variability. 
This suggests that any phenomena giving rise to pressures to increase 
or reduce code churn are perhaps subordinate in magnitude to other, 
more pressing phenomena of local, \ie project-specific, character.

Alternatively, the decreasing trend in commit churn is also consistent with 
the observation by Brindescu \etal~\cite{brindescu2014centralized} that, 
as projects age, bug-fixing commits, which on average are smaller, 
become more prevalent than commits adding new features, which on 
average are larger.

\as{Say something about SE practice/research implications}



%Most of the projects did not show linear trends with time in code churn, on 
%either side of the CI adoption time.
%One explanation consistent with this is that any phenomena giving rise to 
%pressures to increase or reduce code churn are not long term/sustained or, 
%alternatively, perhaps they are subordinate in magnitude to other, more 
%pressing phenomena of local character in time.
%It is also likely that our models and data are insufficient to capture them.
%Some of the reasons for this are: low temporal resolution, high dispersion 
%of the data, non-uniform distribution of the events across time, and project 
%specific considerations that we could not model.

%Our exploratory study showed a peak around CI adoption time and a slight 
%downward trend following CI adoption.
%The modeling study showed that when linear trends were present (in 15\% 
%of the models), the downward one after CI adoption were twice as frequent 
%as the upward one, a reversal of the pattern in the trends from before CI.
%This is consistent with Fowler's recommended good practices of CI, of 
%committing smaller changes to the code.
%Alternatively, it is also consistent with the observation by Brindescu 
%\etal~\cite{brindescu2014centralized} that, as projects age, bug-fixing 
%commits, which on average are smaller, become more prevalent than 
%commits adding new features, which on average are larger.

%The models also showed that when present, the effect of CI adoption on the 
%amount of code churn right after was positive in more cases than negative.
%The increased code churn on both sides near the CI adoption time is 
%arguably in line with expectations that more maintenance work may be 
%going on in preparation for the transition to CI, and that the projects go 
%through some adjustment/cleanup period right after.



%The model fits the data well; it explains 35% of the variability in the data using only the fixed effects (Rm2 = 0.35), and 55% when considering both the fixed and random effects (Rc2 = 0.55).

%deviations in intercept and slope of that subjectï¿½s time trend from the population values.
%(1 + Days | Subject)
%we allow for the possibility that, for example, subjects with higher initial reaction times may, on average, be more strongly affected by sleep deprivation.

%For each project, we fitted a sharp RDD model, as described before.
%Recall that because of the peak we observed, we eliminated the time point 
%immediately before and the one immediately after the CI adoption time, so as 
%they do not throw off the regression models.
%Table~\ref{Table:rddmodels} summarizes the results for the models on the 
%remaining 16 time points.
%Recall that $\beta$ is the slope before, $\gamma$ the size of the effect of 
%CI introduction, $\delta$ is the divergence in the slopes before and after CI 
%adoption, and $\beta + \delta$ the slope of the linear trend after CI adoption.

%Of the 567 models, in 487 there was no significant linear relationship of 
%code churn to time, \ie there was no linear trend.
%In 47 projects there was a rising trend before CI and in 33 projects a falling 
%trend before CI.
%Similarly, in 507 projects no effect of CI introduction on code churn was found. 
%Among the 58 projects where an effect was significant, in 34 the trend was 
%positive, \ie code churn increased, and in 24 it was negative, \ie code churn 
%decreased.
%Interestingly, while in 487 projects no change of slope in the trends before 
%and after CI adoption was found, in the 80 projects with significant change 
%of trends, a reduction in the slope was more prevalent than an increase by 
%about 2 to 1.
%This carried over into trends following CI adoption: out of the 80 significant 
%trend changes, 55 are downward trends, and only 25 are upward.


%Table XX: Projects with statistical negative trends after CI


\bv{I'm here}


%\begin{figure}[t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{churn_before.pdf}
%\caption{Code churn before \Tvis adoption.}
%\label{Fig:CodeChurnBefore}\vspace{-0.3cm}
%\end{figure}
%
%
%\begin{figure}[t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{churn_after.pdf}
%\caption{Code churn after \Tvis adoption.}
%\label{Fig:CodeChurnAfter}\vspace{-0.3cm}
%\end{figure}



\subsection{RQ2: Trends in Commit Frequency}

The second practice we examine is commit frequency.
We examine this question from two directions by looking separately at the non-merge commits at the merge commits. 
%(we obtained the latter as described previously in the Methods section).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/freq-non-merge.pdf}
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/freq-merge.pdf}
\caption{Evolution of mean code churn before and after the \Tvis adoption.}
\label{fig:freq}
\end{figure}


We started from the same data as before, 567 projects, each with at least 500 
non-merge commits.
Additionally, we restricted the projects for the  merge commits study so that we only look at projects that have at least 20 merge commits over the 18 time points, \ie on average about 1 per time point.
This filtering yielded a set of 370 projects for the merge commit frequency study.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{numbercommits.pdf}
\caption{Non-merge commit frequency before and after CI adoption}
\label{Fig:NumberCommits}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth, height=1.5in, clip=true, trim=0 15 15 50]{merges.pdf}
\caption{Merge commit frequency before and after CI adoption}
\label{Fig:MergeCommits}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}: 
To explore general trends over time, we first look at commit frequency in 
the months leading up to CI adoption.
Figs.~\ref{Fig:NumberCommits} and \ref{Fig:MergeCommits} show the boxplots of per-project median 
number of non-merge commits, and merge commits, respectively, for each of nine consecutive 30-day time intervals 
before CI adoption and, respectively, nine consecutive 30-day time intervals 
after CI adoption.
We use a log-scale for the merge commits due to the overdispersion of the data which is sparse over the time periods we have chosen.
As before, the horizontal line in each boxplot is the median value of all 
per-project medians, and the black dot is their average value.

As with code churn, here we also see a peak in the 30-day 
interval just before and the one just after CI adoption.
We do not observe an apparent trend in the 
median or average non-merge commit frequency.
We do observe an upward trend in merge commits, but the variance in the data is large.

\smallskip\noindent \emph{Statistical Modeling Study}: 
As in RQ1, we fitted a sharp RDD model for each project, as described before.
Tables~\ref{Table:rddmodels_freq} and ~\ref{Table:rddmodels_merge_freq} summarize the results.
As before, we eliminated one time point on either side of the CI adoption 
time, so that the peaks at those times  do not throw off the regression models.
\input{tables/rq2_models}

For the non-merge commits, in 452 (80\%) of the models, there was no significant linear relationship 
between  frequency and time, \ie there was no linear trend.
In 71 projects there was a rising trend in commit frequency before CI 
and in 44 projects a falling trend before CI.
Similarly, in 466 projects no effect of CI introduction on commit frequency 
was found. 
Among the 101 projects where an effect was significant, in 59 the trend 
was positive, \ie commit frequency increased, and in the rest it was 
negative, \ie commit frequency decreased.
The mean value over all projects for $\gamma$, the effect size of CI 
adoption, was $1.1$ commit.
While in 454 projects no change of slope in the trends before and after CI 
adoption was found, in the 113 with significant change of trends a reduction 
in the slope was more prevalent than an increase by about 2.5 to 1.
This carried over to a lesser degree into trends in commit frequency 
following CI adoption: out of the 113 significant trend changes, 71 are 
downward trends, and 42 are upward.

For merge commits (Table~\ref{Table:rddmodels_merge_freq}), the results 
are qualitatively similar to those for non-merge commits.

\smallskip\noindent \emph{Discussion}:
As was the case for code churn, most projects did not have longitudinal 
linear trends in non-merge commits, for reasons probably similar to those 
in case of code churn.

Even though  we observed a post-CI adoption increase in merge commits 
in our exploratory study, the statistical tests did not confirm that to be the 
case for more than 20\% of the projects. 
We attribute this disparity to the large variance in the data and especially 
the sparsity of merge commits over time. 

While our exploratory study did not suggest presence of a trend, the 
modeling study suggests that right after CI adoption there is a slight 
positive effect in both the merge and non-merge commits (after removing the peak months around CI adoption time). 
However, this effect is not sustained.
% if a linear trend is present, which is 2.5 times as likely to be decreasing as increasing. 

Given the expected increase in the frequency of commits expressed and 
encouraged by Fowler, our results might appear surprising.
It is possible that some saturation to the commit frequency has already 
been reached even before the CI adoption time.
As we see next, it is also likely that instead developers are spending more time on 
automated testing.


%\as{Vladimir, do you have a cross-combination data, such as \# decrease in churn AND increase in frequency?}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/issues.pdf}
\caption{Evolution of the number of closed issues before and after the \Tvis adoption.}
\label{fig:issues}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/prs-full.pdf}
\caption{Evolution of the number of closed pull requests before and after the \Tvis adoption.}
\label{fig:prs}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth, clip=true, trim=0 0 0 0]{figures/pr-latency-mean.pdf}
\caption{Evolution of the mean PR latency before and after the \Tvis adoption.}
\label{fig:pr-latency}
\end{figure}


\input{tables/freq-model}
\input{tables/issues-model}
\input{tables/latency-model}



\subsection{RQ3: Trends in Issue Report Frequency}

The 3rd practice we examine is issue report frequency.
As above, we use data on 293 projects, each with 
100+ issues.

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{issues_before.pdf}
\caption{Median number of issues before CI adoption}
\label{Fig:IssuesBefore}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{issues_after.pdf}
\caption{Median number of issues after CI adoption}
\label{Fig:IssuesAfter}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}:
We follow the same approach as in RQ1 and RQ2 and compare the 
medians and the averages in the number of issues per time period, in the 
months before and after CI adoption.
Fig.~\ref{Fig:IssuesBefore} and \ref{Fig:IssuesAfter} plot the number of 
issues per unit time period.
Observing these figures shows that the months immediately preceding 
and following the adoption of CI exhibit the highest number of issues.
Besides those, before CI adoption the median number of issues per period seem to vary between 2 and 4, with most of them being at 3, while the averages are between 5 and 6.
Following CI adoption the median of the number of issues seems to 
stabilize at 4 per period, and the averages are between 6 and 7.

\input{tables/rq3_models}

\smallskip\noindent \emph{Modeling Study}:
Results of the RDD modeling are summarized in Table~\ref{Table:rddmodels_issues}.
The time period before and the one after the time of CI adoption have been removed from the modeling, as explained in RQ1 above.
The lower number of projects in Table~\ref{Table:rddmodels_issues} 
compared to Tables~\ref{Table:rddmodels} and~\ref{Table:rddmodels_freq} 
should not be surprising. 
Recall from Section~\ref{sec:dd} that to answer RQ3 we have focused on 
projects having at least 100 issues, further reducing the size of the data set 
used in RQ1 and RQ2 to 293.
We have fitted models to 291 projects of the 293, however in most cases we 
did not observe linear trends.
When a linear trend has been observed than it was more than twice more 
often decreasing than increasing.
The mean value over all projects for $\gamma$, the effect size of CI adoption, 
was $1.0$ issue.


\smallskip\noindent \emph{Discussion}:
Again, most of the projects did not have longitudinal linear trends for reasons probably similar to those in the case of the code churn.

As in RQ1, our exploratory study showed a peak in the median number of issues per period around CI adoption time, and an upward trend following CI adoption.
The modeling study showed that when linear trends were present (in 20\% 
of the models), the downward one after CI adoption were 2.5 times as frequent 
as the upward one, a reversal of the pattern in the trends from before CI.
This has to be considered in combination with the size of the effect of CI adoption, which is $1$ issue per period additional.
Thus, the number of issues increases significantly overall after CI adoption, to the tune of $33\%$, and then has a pressure of slight reduction over time.

This is consistent with an increased number of issues brought up after CI adoption. Some of those are invariably related to the change in the way things are done, and the transition. Others may be related to the increased automated testing which likely discovers more concerns and bugs, which are then mentioned in issues.
Overall, this result shows that the adoption of CI brings about more issues, as expected.




\subsection{RQ4: Trends in Testing}

The last development practice we examine is testing and the evolution 
of the types of errors revealed by automated testing.
The data consists of 250 projects, each with at least 100 builds, and 90\% 
of builds executing tests.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{tests.pdf}
\caption{Unit tests per build following CI adoption}
\label{Fig:Tests}
\end{figure}

\smallskip\noindent \emph{Exploratory Study}: 
As before, we first look for general trends over time.
Fig.~\ref{Fig:Tests} shows a boxplot of per-project median number of tests 
per build, for each of five consecutive 60-day time intervals before CI adoption.
We aggregated the data here in 60 day intervals to make it easier to visualize 
the trend, since the differences are small.
The horizontal line in each boxplot is the median value of all per-project medians, 
and the black dot is their average value.
We observe a monotonically increasing trend in both the medians, from 140 to 
160 tests per build, and the means, from 205 to 245 tests per build, \ie 15\% to 
20\% increase. 

To ascertain if the complexity of builds increases over time, we also calculated 
the average number of jobs per build.
The median is $1$ for all five time intervals.
These two findings suggest strongly that there is an increase in the number of 
unit tests per build over time.
This, coupled with our finding that builds are not getting more complex over time indicates that developers are likely spending more time on 
automated testing since CI adoption.
This is consistent with Fowler's ``good practices" proposal.



\smallskip\noindent \emph{Error Types Study}:
We also looked at the evolution of the error types in builds over time after CI 
adoption.
For all 250 projects above, we looked at the builds that resulted in errors, 
and deconvolved the errors into 8 types, as described above.
We did this over 3 intervals: 0-60 days, 61-180 days, and 181-300 days (\ie 
corresponding to the first, third, and fifth interval in Fig.~\ref{Fig:Tests}).
The results are shown in Fig.~\ref{Fig:BugTypes}, where on the y-axis are 
the proportion of all erroneous builds that yield that error type.
We find an apparent upward trend over time in most error type categories, 
most notably compile errors, execution errors, failed tests, and skipped 
test.\footnote{The median number of error types per build remained $2$ over 
time.}
All these are consistent with an increase in the amount of code being built 
and tested per build, as well as an increasing management of errors by 
skipping tests, likely to aid in debugging.
On the other hand, we find a decreasing trend among errors related to missing 
files/dependencies, and time-outs, both consistent with those errors being less 
of an issue over time, as developers are acculturating to the CI mediated 
processes.

Thus, overall we find an expected adjustment to the automated testing and 
error handling, with an indication that debugging complexity grows over time.

%%this is the old plots
%\begin{figure}[!t]
%\centering
%\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{plot_together.png}
%\caption{Evolution of Error Types Since CI Adoption}
%\label{Fig:BugTypesold}
%\end{figure}


%this is the new plots
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth, clip=true, trim=0 15 15 50]{new_plot_together.png}
	\caption{Evolution of Error Types Since CI Adoption}
	\label{Fig:BugTypes}
\end{figure}

