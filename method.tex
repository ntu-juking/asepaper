% !TEX root = CI_Adoption.tex

\section{Methods}
\label{sec:method}

We collected and statistically analyzed data from a sample of open-source 
Java (language chosen based on familiarity of the first author) projects on 
\GH, that adopted \Tvis at some point during their history.

\subsection{Data Gathering}

Data collection involved mining multiple sources, the \GH API, project version 
control (git) logs, and the \Tvis API.
The goal was to select non-trivial projects that adopted \Tvis, and had sufficient
activity both before and after adoption, in order to observe potential transition effects.
Note that for the purpose of this study we don't distinguish between ``project'' 
and ``repository''; the term ``project'' has also been used to refer to a collection 
of interrelated repositories in the literature~\cite{vasilescu2016sky}.

\smallskip\noindent\emph{Candidate projects and initial filtering:} 

We started by composing (using the \GH Search API) a list of candidate Java 
projects that: (i)~were not very small (a large fraction of projects on \GH are 
small and inactive~\cite{gousios2014exploratory}; we arbitrarily chose to ignore
projects smaller than 500 kilobytes, using the \GH Search API's \emph{size} 
parameter); and (ii)~were created no later than 2014-11-08. 
The second criterion is a consequence of our data collection date (2016-05-01)
and the ``sufficient history'' requirement. 
Our statistical analysis, detailed below, assumes 9 months of history for each
project both prior to and after adopting \Tvis; since we did not have access to
data on when each project started using \Tvis at this stage, we conservatively
chose the 2014-11-08 date, which allows 9*30*2 days of history from project 
creation to 2016-05-01.
This step resulted in a list of over 300,000 candidate Java projects.

%first and foremost, we should make sure our studied projects have sufficient history (e.g. 9*30 days) both before using Travis-CI and after using Travis-CI.  As the data were extracted from the inception until 2016-05-01, the projects selected should be created at least before 2014-11-08, so that there are at least 9*30*2 days from its creation to 2016-05-01. 
%From GitHub Search API, we first found over 300k Java projects created before 2014-11-08 and with $\geqslant 500$ kilobytes. 
%After consulting Travis-CI, we further identify projects that: 1) adopted Travis-CI; 2) at least 9 * 30 days from project creation to Travis-CI adoption; 3) at least 9 * 30 days from Travis-CI adoption to 2016-05-01. This filtering process left us 1566 projects.  
%For each project, we collect the whole history of commits, issues, and pull requests from GitHub API, and the history of Travis-CI builds and job logs from Travis-CI repository using the ruby library of \textit{travis gem}.  In addition to these publicly available data, we further gathered the information about the number of tests and types of errors during the Travis-CI run.

Next, we used the \Tvis API to determine which of these projects:
(i)~used \Tvis, and if yes, we recorded the date of the earliest build as the 
adoption date; (ii)~had at least 9*30 days of history from project creation to 
the adoption date, and from the adoption date to 2016-05-01, respectively. 
This filtering process resulted in 1,566 projects.
For each project, we collected: (i)~its history of commits, issues, and pull 
requests using the \GH API; and (ii)~its history of \Tvis builds and, for each
build, all its job logs (a \Tvis build can be configured to run multiple jobs,
one for each set of user-defined configuration options; each job run produces 
a ``build log''), using the \Tvis API and the \textit{travis gem} Ruby library.  

\smallskip\noindent\emph{Mainline commits:}

Since git is a distributed version control system, developers can work locally,
in increments, before pushing their changes to \GH or opening a pull request.
E.g., a developer can choose to partition a large change into many smaller 
ones, and make multiple local commits to better handle the work.
This would not have any effect on CI runs, since CI is only triggered by \GH 
push events, and events happening after (and including when) a pull request 
is opened.
%Pull requests are also a mechanism to isolate individual developers from 
%activity on the mainline.
Consequently, to study \emph{Commits To the \_Mainline\_ Every Day}
(Fowler's best practice), as opposed to potentially local git commits, we
distinguish between \emph{non-merge commits} and \emph{merge commits}
as a proxy.
We use  \texttt{\small git rev-list --all --merges} to recognize merge commits.

%Starting from RQ1 and RQ2 the practice being investigate should be �Commits To the Mainline Every Day�, but then you analyze git commits. Not surprisingly, commits do not increase. Why they should? The main reason of having many commits is the ability to partition a large change in many smaller ones and to better handle the work locally. What really matters for the CI are pushes and pull requests. Indeed, I would expect an increase of pushes/pull requests when CI is being adopted. Maybe a way to observe this is to actually analyze merge commits and see whether their frequency increases.
%Similar considerations apply for the size of code changes. Also, I�ve the feeling that such a size may strongly depend on the kind of activity being performed. For example, it could be that bug fixing correlates with small change size, whereas feature addition with larger ones. 

\smallskip\noindent\emph{Bug fixing vs.\ other activities:} 

The kind of activity being performed may confound commit size, \eg since one 
can expect bug fix commits to be smaller than new feature implementations.
We additionally annotate the non-merge commits as \emph{bug-fixing} or not, 
by searching for typical patterns in commit 
messages~\cite{mockus2000identifying}.\footnote{E.g., \verb$^(bug)?fix(es|s|ed|ing)?(bug)?([[:punct:]]|\\s+)$}
Note that we don't enforce that commit messages that match our regular
expressions also contain references to existing issue report numbers, in an
attempt to be more robust to different issue trackers (indeed, not all projects 
use \GH's issue tracker) and different project linking / commit message conventions.
%
%The "fix" heuristic without issue tracker links (looking for issue numbers and verifying they exist in the issue tracker) has the disadvantage that it will detect "false positives", i.e., commits made not in response to an open issue; plus the extra disadvantage that it will identify commit messages containing "suffix", for example.
%
%However, not all projects use GitHub's issue tracker, as we know and a reviewer pointed out. Also, the original intent, triggered by a review comment, was to distinguish between new features and bug fixes. We can say that the "fix" heuristic more closely accomplishes this by being robust to issue trackers and project linking conventions. The only downside I can think of is if people close open "feature request" issues from the issue tracker, using "fix" language.




\input{tables/ouput_examples}

\smallskip\noindent\emph{Number of tests executed per build:} 

We also parsed the \Tvis build logs and extracted information about the 
number of tests executed and the types of reasons causing builds to break.
%We provide more details next.
Each \Tvis build consists of at least one job, which corresponds to a particular
build \& test environment (\eg jdk version, environment variables). 
Once the job is started, a log is generated, recording the detailed information
of the build lifecycle, including installation steps and output produced by the 
build managers. % scripts for testing. 
Table~\ref{log_example} shows fragments of the logs generated by the 
three widely used Java build tools, Maven, Ant, and Gradle. 
To investigate the evolution in testing practices across builds, we wrote a tool 
to analyze the logs and extract summary information about test executions.  
Since the relationship between builds and jobs is one-to-many, we use the 
maximum number of tests across jobs as the test count for the build. 

\input{tables/error_types}

\smallskip\noindent\emph{Error classification for failed builds:}

\Tvis marks a build as \textit{failed} if at least one of its jobs not explicitly 
labeled as \emph{allowed to fail} in the project's CI configuration 
file\footnote{\url{https://docs.travis-ci.com/user/customizing-the-build/#Rows-that-are-Allowed-to-Fail}} failed.
Therefore, to understand why \Tvis builds failed, we proceed to find out 
what happened in the jobs. 
We first manually reviewed a set of failed jobs to recognize what kinds of 
failures occur and whether there are any corresponding textual patterns in 
the logs.
Then, we extracted a set of mapping rules to classify these patterns into 
categories.
We proceeded iteratively by inspecting more randomly selected failed jobs
and gradually refining our classification scheme, until reaching saturation. 
%To mitigate the risk of bias arising from missing and incorrect classification, 
%we augmented the manual reviewing rates to improve our classification 
%rules and remove spurious classification as much as possible. 
%The classification scheme evolved during this process, and was gradually 
%refined to cover more textual patterns. 
In the end, we identified 8 categories of reasons for build failures, 
as summarized in Table~\ref{error_types}. 
%a group of mapping rules to classify the errors into 8 types, as shown 

Next, we implemented tools for automatic classification. 
The detailed process comprises the following steps:

\noindent 1)~\emph{Failure Location}. 
We first recognized failed jobs with \textit{non-allow-failure} attributes, which 
result in the build's final \textit{failed} state. 
Then, we attempted to locate the command which caused the breakdown 
in the log file, as the command which exited with a non-zero value.
The log entry for this command usually provided detailed information about 
the fatal errors that occurred. 
If such commands were not recorded in the log file, we expanded the search 
scope to the \textit{script} phase and the \textit{after script} phase of the 
build log, as per \Tvis build lifecycle.\footnote{\url{https://docs.travis-ci.com/user/customizing-the-build/#The-Build-Lifecycle}}
Note that if errors occur in the \emph{before script} phase of the build, the 
job is automatically marked as \textit{errored} and stops immediately. 
Only if the \textit{script} phase returns a non-zero exit code, or the \textit{after 
script} phase times out, then the job is marked as \textit{failed}. 
Therefore, we first check if there is a time-out error in \textit{after script}. 
If not, then it's likely that the errors in the \textit{script} phase caused the 
build to break. 

\noindent 2)~\emph{Log Parsing \& Tagging}. 
After extracting the log fragment describing the errors, we 
used textual pattern matching for classification. 
%
%\noindent 3)~\emph{Tagging}. 
%We tagged the extracted textual patterns with a subset of the defined 8 error 
%types. 
Note the one-to-many relationship between jobs and failure types. 
E.g., if a job had both failed tests and skipped tests, it is tagged 
with both ``failed test'' and ``skipped or pending test'' error types.


%\textit{Distinguishing the bugs introduced before CI adoption and the bugs introduced after CI adoption}: 
%We suppose that the defects before and after introducing Travis-CI are different. To verify this, we use the SZZ algorithm to gather bug data, and divide them into two groups ( \ie bugs introduced before Travis-C and bugs introduced after Travis-CI ).
%In GitHub, when a bug is reported, an issue will be created to track this bug, and subsequently a set of commits occur for bug fixing. 
%We decide whether a issue is a \textit{fix issue} based on the constrains that 1) the issue has been taged as a bug (with labels like \textit{bug}, \textit{defect}, \textit{fault}); 2) the issue has at least one commit with source file modification to fix this bug. To do this, We first analyze the commit messages to recognize its corresponding issue based on the special textual patterns (\eg fix \textit{issue\_id}, close \textit{issue\_id}, gh-\textit{issue\_id}), and then check if this issue has bug tag. If yes, we will use git diff to find the change details (\ie changed files and lines), and further check if the commit has changed source files.
%After the above conditions checking, we identify fix issues and the corresponding fix commits. The lines changed in fix commits are targeted to fix the bug. Using \textit{git blame}, we can locate which commit last added or modified these lines. We consider this commit as a buggy commit as it introduced bug. 
%As we know, a fix issue will only be triggered by the bug introduced before the issue is opened. Therefore, a necessary condition for filtering is that the buggy commit must have been pushed before the bug being reported (\ie issue being created), otherwise, this buggy commit should not be implicated.
%With above process, the bugs can be divided into two groups based on when the bugs were introduced (\ie buggy commit time) and when the project started using Travis-C. For each group, we gathered the bug logs from the fix issues and fix commits, and then built word cloud to analyze the differences between them.

%\subsection{Overview of the Data Set}
\subsection{Additional RQ-Specific Filtering}
\label{sec:dd}

\input{tables/projs_summary}

%Our study makes use of a large-scale data set collected from the selected 1566 java projects. 
Table~\ref{projs_summary} contains descriptive statistics for the selected 
1,566 Java projects that adopted \Tvis. 
We note an average number of 347 builds per project. 
We further census the builds in terms of the triggering event type and final 
state (Table~\ref{builds_count}). 
Out of 541,959 builds total in our data set, 68\% came from push commits 
and 32\% from pull requests (PRs). 
Of these builds, 67.1\% passed, 18.6\% failed, and the others errored or were canceled. 
In addition, Table~\ref{count_info} lists statistics for \#Commits, \#Issues, 
and \#PRs before and after adopting \Tvis, respectively. 
We can preliminarily see that there are fewer commits 
after adopting CI, but more issues and PRs.
From the summary statistics, we found there are large variances between 
projects in terms of each attribute (Table~\ref{projs_summary}).
For example, 18\% of projects have no \GH issues;
% a value of \#issues equal to 0, as they 
%haven't used \GH issue tracking yet. 
when studying the evolution of issue reporting practices, these projects without 
issues may bias our conclusions. 
As a result, to avoid too many zeros in our subsequent time series analysis, 
we did more data filtering for each RQ individually, as follows.
%based on different conditions, 


For RQ1 and RQ2, we study code churn and commit frequency. 
To reduce potential bias due to data sparsity (since we aggregate data at 
monthly intervals; see Section~\ref{sec:tsa}), we discard projects having 
fewer than 500 total non-merge commits (arbitrarily chosen).
%In our data set, 8.9\% of commits are merge commits. 
%First, we remove these merge commits, to avoid double counting and since 
%they were automatically generated. 
%Then, we further filter the projects based on the number of non-merge commits,
%to reduce potential bias due to data sparsity (since we aggregate data at 
%monthly intervals), and discard projects with less than 500 total non-merge 
%commits.
%%We Each project should have at least 500 nonmerge commits. 
This restricts the data set to 567 projects, with a 
total of 1,629,090 non-merge commits (of them, 274,410 are fix commits) 
and 157,976 merge commits.

For RQ3, we only selected projects with at least 100 issues for similar sparsity 
avoidance reasons. 
The resulting filtered data set contains 293 projects (143,573 \GH issues total).

For RQ4, we investigate changes in \#tests per build.
As introduced above, we collected \#tests from build logs. 
First we filtered the projects based on the number of builds
(lower bound arbitrarily set at 100), leaving 736 projects. 
%Each projects should have at least 100 builds. This selection left us 736 projects. 
Furthermore, during the collection of \#tests, we found (and subsequently
filtered out) 219 projects that did not execute any tests as part of their \Tvis 
builds, and additional 267 projects that tested scarcely (\ie we kept 
projects for which at least 90\% builds executed at least one test). 
%We just removed these projects as they didn't have \# test values. 
In the end, 250 projects remained in the filtered RQ4 data set.
%Among the other projects, 250 of them have more than 90\% builds with at least one test. Here, we only consider these 250 projects with high coverage of testing ( \textgreater{ 90\%} ).

\subsection{Time Series Analysis Method}
\label{sec:tsa}

We use data visualization and statistical modeling to discover 
longitudinal patterns indicative of CI adoption effects on development practices.
As one of our contributions, we introduce the statistical modeling 
framework of \emph{regression discontinuity design}~\cite{imbens2008regression} 
to assess the existence and extent of a longitudinal effect of CI adoption on 
development practices.

To evaluate the effect of a treatment, \eg new drug on a disease progression, 
randomized experimental trials are conducted, 
in which the experimental 
cohort is randomly split into a treatment group, \ie those given the treatment, and 
a control group, \ie those not given the treament. 
Then, the effect is evaluated based on the difference in disease progression 
between the two groups.
In the absence of randomized trials, as is the case with trace data in 
empirical software engineering, weaker techniques which make 
additional assumptions, \eg quasi-experiments, are employed.

Regression discontinuity design (RDD)~\cite{imbens2008regression} is an example of such a technique, 
used for modeling the extent of a discontinuity of a function between its values 
at points just before and just after a given intervention. 
RDD is based on the assumption that in the absence of the intervention, the 
trend of the function would be continuous in the same way as prior to it.
Fig.~\ref{RDDIllustration} illustrates a discontinuity, and the RDD approach, in a nutshell, aims to uncover the 
 different regression lines before and after the discontinuity.

There are a number of different formalizations of RDD, most prominently 
sharp RDD and fuzzy RDD~\cite{imbens2008regression}.
%Each, in turn, can be implemented in a variety of ways.
To model the effect of CI adoption on developer practices, here we chose one implementation
of the simpler, sharp RDD approach: linear regression with an interaction term.
We summarize our approach next, following the description by Imbens and 
Lemieux~\cite{imbens2008regression}, and refer to Figure~\ref{RDDIllustration}.
Let $Y$ be the outcome variable in which we are looking for a discontinuity, 
\eg change in commit churn per month before vs after CI adoption, let $X$ be the temporal variable containing the 
time of intervention, and let $c$ be the time point at which the intervention, \eg CI adoption, has occurred.
Then, an RDD model for points $x_i$ in equal intervals $h$ on each side of 
$c$, $c-h \le x_i \le c+h$, is given by:

\[y_i \ = \alpha + \beta(x_i-c) + \gamma w_i + \delta(x_i-c)w_i + \epsilon_i,\]

\noindent where $w_i = (x_i \geq c)$, \ie $w_i$ is 1 if point $x_i$ is included in 
the treatment group (\eg after CI adoption), and 0 if it is before the treatment.
In fact, this model encapsulates two separate regressions.
For points before the treatment, the resulting regression line has a slope of 
$\beta$, and after the treatment $\beta + \delta$.
The size of the effect of the treatment is the difference between the two 
regression values of $y_i$ evaluated at $x=c$, and can be seen to be equal 
to $\gamma$.
For example, in Fig.~\ref{RDDIllustration}, the treatment effect ($\gamma$) is negative, and there is an interaction effect ($\delta \neq 0$) which changes the slope of the regression after the treatment.
A critical assumption for RDD is that but for the treatment, all other variables remain steady, before, during, and after the treatment.

For each project, we use an RDD model implemented as the above 
double linear regression, on data centered at the time of CI adoption, and 
having equal number of points on each side.
Solving the regression gives us the coefficients, which if significant, can help us reason about the treatment and its effects, if any.
We report on the models having significant coefficients in the regressions (<0.05). Our succesful models had an $R^2$ of at least $0.65$. 



\begin{figure}[t]
	\centering
	\includegraphics[width=0.4\textwidth, clip=true, trim=0 15 15 50]{RDD_plot.png}
	\caption{RDD illustration.}\vspace{-0.5cm}
	\label{RDDIllustration}
\end{figure}

