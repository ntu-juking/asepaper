% !TEX root = CI_Adoption.tex

\section{Methods}
\label{sec:method}

\subsection{Data Gathering}

To perform our study, first and foremost, we should make sure our studied projects have sufficient history (e.g. 9*30 days) both before using Travis-CI and after using Travis-CI.  As the data were extracted from the inception until 2016-05-01, the projects selected should be created at least before 2014-11-08, so that there are at least 9*30*2 days from its creation to 2016-05-01. 
From GITHUB Search API, we first found over 300k Java projects created before 2014-11-08 and with $\geqslant 500$ kilobytes. After consulting Travis-CI, we further identify projects that: 1) adopted Travis-CI; 2) at least 9 * 30 days from project creation to Travis-CI adoption; 3) at least 9 * 30 days from Travis-CI adoption to 2016-05-01. This filtering process left us 1566 projects.  
For each project, we collect the whole history of commits, issues, and pull requests from GITHUB API, and the history of Travis-CI builds and job logs from Travis-CI repository using the ruby library of travis gem.  In addition to these publicly available data, we further gathered more information as follows.

Number of tests: 
Each build has at least one job which indicates an execution of the build on a specified environment. Once the job is started, a tracking log will be generated, which records the detailed information of the build lifecycle including the installation steps and the build scripts for testing. To investigate the testing evolution across builds, we attempt to mine data from these logs . We build an automatic tool to analyze Travis-CI build logs and extract summary information about test executions. 
Since the relationship between builds and jobs are one-to-many, we use the maximum number of tests as the test count for the build. 

\input{tables/error_types}
Error classification for failed Travis-CI builds: 
In Travis-CI, a build will be marked as failed if one of its \textit{non-allow-failure} jobs failed. From this point, to understand why the Travis-CI builds failed, we attempt to find out what happened in the jobs. We first manually reviewed a set of failed jobs to recognize what kinds of failures occur and the corresponding used textual patterns, and then extracted a group of mapping rules to classify these patterns into categories. To mitigate the risk of bias arising from missing and incorrect classification, we augmented the reviewing rates to improve our classification rules and remove spurious classification as much as possible. The classification scheme evolved over the process of manual review, and was refined gradually to cover more textual patterns. Finally, we summarize over 100 mapping rules to category the errors into 7 types, as shown in Table~\ref{error_types}. 
We implemented tools to do the classification automatically .The detailed process is that: 1) failure log location. We first recognized failed jobs with \textit{non-allow-failure} attribute, which result in the build's final state. Then we locate the errors which may indeed cause the breakdown. From the log file, we find the failed command with non-zero return value, which is the direct cause of the job’s breakdown, and the log of this command usually describe the detailed information about the deadly errors happened. If such commands were not recorded in the log file, we will expand search scope to the \textit{script} phase and the phase \textit{after script} based on Travis-CI build lifecycle. Because if errors occur before script phase, the job will be marked as \textit{errored} and stops immediately. Only if the \textit{script} phase returns a non-zero exit code or the phase \textit{after script} times out, the job will be marked as \textit{failed}. So we first check if there is a time-out error after script. If not, the errors happened in the \textit{script} phase potentially caused the build broken. 2) log parsing. We use textual pattern matching method to recognize the errors resulting in the job failure. 3) classification. After extracting the textual patterns, we categorize them  into the defined 7 types based on the rules. There will be one to many relationship between jobs and types. For example, if a job have failed tests and skipped tests, it will be tagged with both “failed test” and “skipped or pending test” types.

\textit{Distinguishing the bugs introduced before CI adoption and the bugs introduced after CI adoption}: 
We suppose that the defects before and after introducing Travis-CI are different. To verify this, we use the SZZ algorithm to gather bug data, and divide them into two groups ( \ie bugs introduced before Travis-C and bugs introduced after Travis-CI ).
In Github, when a bug is reported, an issue will be created to track this bug, and subsequently a set of commits occur for bug fixing. 
We decide whether a issue is a \textit{fix issue} based on the constrains that 1) the issue has been taged as a bug (with labels like \textit{bug}, \textit{defect}, \textit{fault}); 2) the issue has at least one commit with source file modification to fix this bug. To do this, We first analyze the commit messages to recognize its corresponding issue based on the special textual patterns (\eg fix \textit{issue\_id}, close \textit{issue\_id}, gh-\textit{issue\_id}), and then check if this issue has bug tag. If yes, we will use git diff to find the change details (\ie changed files and lines), and further check if the commit has changed source files.
After the above conditions checking, we identify fix issues and the corresponding fix commits. The lines changed in fix commits are targeted to fix the bug. Using \textit{git blame}, we can locate which commit last added or modified these lines. We consider this commit as a buggy commit as it introduced bug. 
As we know, a fix issue will only be triggered by the bug introduced before the issue is opened. Therefore, a necessary condition for filtering is that the buggy commit must have been pushed before the bug being reported (\ie issue being created), otherwise, this buggy commit should not be implicated.
With above process, the bugs can be divided into two groups based on when the bugs were introduced (\ie buggy commit time) and when the project started using Travis-C. For each group, we gathered the bug logs from the fix issues and fix commits, and then built word cloud to analyze the differences between them.

 
b) data description
\input{tables/projs_summary}

Our study makes use of a large-scale data collected from the selected 1566 java projects. Table~\ref{projs_summary} summarizes the descriptive information, including the min, median, mean and max values. All these projects have adopted Travis-CI, with an average number of 347 builds. We further census the builds based on the evert type and finaly state. Table ~\ref{builds_count} show that, there are total 541959 builds in our data set, 68\% come from push and 32\% come from pull request. Of these builds, most were passed (67.1\%), 18.6\% were failed, and the others were errored or canceled. Besides, Table ~\ref{count_info} lists the statistics for \# commits, \# issues and \# PRs before and after adoptiong CI respectively. We can see that, in our studied projects, there are less commits after CI adoption, but more issues and PRs. From the summary statistics, we found these projects are in a large range of values in terms of each attribute, as shown in Table ~\ref{projs_summary}. For example, 18\% projects haven't featured issue tracking yet. When studying the evolution practices on issues, these projects without issues will bias our conclusion. As a result, we did more data filtering for each RQ individually based on different conditions as follows.

For RQ1,we study the code churn of commits. First we remove the merge commits, because merge commits are automatically generated by Git by default. In our data set, 8.9\% commits are merge commits. After removing them, we further filter the projects based on the number of nonmerge commits. Each project should have at least 500 nonmerge commits. Finally, the data for RQ1 consists of 567 projects with total 1629090 nonmerge commits.

For RQ2,  we investigate the changes in \# tests per build.
As introduced above, we collected \# tests from build logs. So we first filtered the projects based on the number of builds. Each projects should have at least 100 builds. This selection left us 736 projects. 
During the collection of \# tests for each build, we found that, in 250 projects, more than 90\% builds have at least one test. We also found 219 projects do not deploy test excecution.  Here, we only consider the projects with high coverage of testing. Therefore, the data set for RQ2 contains 250 projects, each has at least 90\% of builds with test excecution. 

For RQ3, we selected the projects with at least 100 issues. And then we got 293 projects with total 143573 issues.


c) Time Series Analysis Methods

We use data visualization and statistical modeling methods to discover longitudinal patterns indicative of CI adoption effects on development practices.
As one of the contributions of this paper, we introduce the statistical modeling framework of {\emph regression discontinuity design} to assess the existence and extent of a longitudinal effect of CI adoption on development practices.

To evaluate the effect of a treatment, e.g., a new drug on a disease progression, randomized experimental trials are typically used, in which the experimental cohort is randomly split into a treatment group, i.e. those given the drug, and a control group, i.e., those given an experientially identical placebo. Then, the effect is evaluated based on the difference in disease progression between the two groups.
In the absence of randomized trials, as is the case with trace data common in empirical software engineering, weaker techniques, e.g., quasi-experiments, are employed which make additional assumptions.

Regression discontinuity design (RDD) is an example of such a technique, used for modeling the extent of a discontinuity of a function between its values at points just before and just after a given intervention point. 
It is based on the assumption that in the absence of the intervention, the trend of the function would be continuous.
A common situation where a discontinuity occurs is illustrated in Fig.~\ref{RDDIllustration}, where the discontinuity is shown as occuring at the time of the intervention, and is manifested as a different regression before and after that point.

There are a number of different formalizations of RDD, most prominently sharp RDD and fuzzy RDD.~\cite{}
Each can, in turn, be implemented in a variety of ways.
To model the effect of CI adoption on developer practices, here we chose one of the simpler approaches of linear regression with an interaction term.
We summarize next our approach, following the description from a recent review of RDD~\cite{}. We refer to Figure~\ref{RDDIllustration} for an illustration.
Let $Y$ be the outcome variable in which we are looking for a discontinuity, e.g. commit churn per month, let $X$ be the temporal variable containing the time of intervention, and $c$ be the time point at which the intervention, in this case CI adoption has happened.
Then, an RDD model for points $x_i$ in equal intervals $h$ on each side of $c$, $c-h \le x_i \le c+h$, is given by:

\[y_i \ = \alpha + \beta(x_i-c) + \gamma w_i + \delta(x_i-c)w_i + \epsilon_i,\]

\noindent where $w_i = (x_i > c)$, i.e. $w_i$ is 1 if point $x_i$ is included in the treatment group (e.g., after CI adoption), and 0 if it is before the treatment.
In fact, this model encapsulates two separate regressions.
For points before the tratment, the resulting regression line has a slope of $\beta$, and after the treatment $\beta + \delta$.
The size of the effect of the treatment is the difference between the two regression values of $y_i$ evaluated at $x=c$, and can be seen to be equal to the intercept $\alpha$.

For example, in Fig.~\ref{RDDIllustration}, ...

For each project, we use a sharp RDD model implemented as the above double linear regression, on data centered at the time of CI adoption, and having equal number of points on each side.
Solving the regression gives us the coefficients, which we can reason about, especially the effect size, if any.
 




