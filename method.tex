% !TEX root = CI_Adoption.tex
\section{Methods}
\label{sec:method}

a) data gathering

From GITHUB Search API, we first find over 430k Java projects created before 2015-05-01 and with size larger than 500 kilobytes. After consulting Travis-CI API, we further identify projects that: 1) adopted Travis-CI; 2) at least 9 months before and 9 months after Travis-CI adoption. After this filtering process, our data set contains 1581 projects.  
For each project, we collect the whole history of commits, issues, pull requests and comments from GITHUB API, and the history of Travis-CI builds and job logs from Travis CI API.  The data were extracted from the inception until 2016-05-01. 
In addition to these publicly available data, we further gathered more information as follows.

Number of tests: 
Each build has at least one job which indicates an execution of the build on a specified environment. Once the job is started, a tracking log will be generated, which records the detailed information of the build lifecycle including the installation steps and the build scripts for testing. To investigate the testing evolution across builds, we attempt to mine data from these logs . We build an automatic tool to analyze Travis-CI build logs and extract summary information about test executions. 
Since the relationship between builds and jobs are one-to-many, we use the maximum number of tests as the test count for the build. 
To investigate the \#tests per build, we select the projects based on \#builds.  we did the \#tests gathering on 736 projects with at least 100 builds. We found that 219 of them do not deploy test excecution, and in 250 projects, over 90\% builds have at least one test. 

Error classification for failed Travis-CI builds: 
In Travis-CI, a build will be marked as failed if one of its \textit{non-allow-failure} jobs failed. From this point, to understand why the Travis-CI builds failed, we attempt to find out what happened in the jobs. We first manually reviewed a set of failed jobs to recognize what kinds of failures occur and the corresponding used textual patterns, and then extracted a group of mapping rules to classify these patterns into categories. To mitigate the risk of bias arising from missing and incorrect classification, we augmented the reviewing rates to improve our classification rules and remove spurious classification as much as possible. The classification scheme evolved over the process of manual review, and was refined gradually to cover more textual patterns. Finally, we summarize over 100 mapping rules to category the errors into 7 types, as shown in Table?. 
We implemented tools to do the classification automatically .The detailed process is that: 1) failure log location. We first recognized failed jobs with \textit{non-allow-failure} attribute, which result in the build's final state. Then we locate the errors which may indeed cause the breakdown. From the log file, we find the failed command with non-zero return value, which is the direct cause of the job’s breakdown, and the log of this command usually describe the detailed information about the deadly errors happened. If such commands were not recorded in the log file, we will expand search scope to the \textit{script} phase and the phase \textit{after script} based on Travis-CI build lifecycle. Because if errors occur before script phase, the job will be marked as \textit{errored} and stops immediately. Only if the \textit{script} phase returns a non-zero exit code or the phase \textit{after script} times out, the job will be marked as \textit{failed}. So we first check if there is a time-out error after script. If not, the errors happened in the \textit{script} phase potentially caused the build broken. 2) log parsing. We use textual pattern matching method to recognize the errors resulting in the job failure. 3) classification. After extracting the textual patterns, we categorize them  into the defined 7 types based on the rules. There will be one to many relationship between jobs and types. For example, if a job have failed tests and skipped tests, it will be tagged with both “failed test” and “skipped or pending test” types.


b) data description

c) analysis methods

- Regression discontinuity design




- Statistical models

- boxplots

- case studies