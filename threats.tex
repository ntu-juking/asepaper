% !TEX root = CI_Adoption.tex

\section{Threats to Validity}
\label{sec:threats}
In this section, we discuss the threats to construct validity, internal validity  and external validity~\cite{perry2000empirical}.

\textbf{Construct Validity} assesses whether the variables we considered accurately model the constructs of our study. 
One of the constructs in our study is ``CI adoption time'' that we operationalize as the ``first build of Travis CI''. 
A more precise definition of ``CI adoption time'' would have involved results of thee actions: registration of the repository with Travis CI, build execution and commit involving \texttt{.travis.yml}.
Information about the repository registration is not publicly available. 
Travis-CI can create a build using the default settings, without \texttt{.travis.yml} being present in the repository.
However, by default Travis-CI is configured for Ruby, and obviously any attempt to build a Java project as if it were Ruby are deemed to fail.
%When registering the project one might also indicate that the builds should be created only if \texttt{.travis.yml} is present.
Similarly, after \texttt{.travis.yml} is committed, Travis-CI does not necessarily create a build since the repository has to be registered and changes have to be pushed first. 
Most projects perform the three actions on the same day, or at least within a couple of days.
However, in several projects such as \as{Bogdan, what was the name of this SuperAwesome thing?} completing the three actions has taken more than \as{Bogdan, please check} years.
Hence, validity of the ``CI adoption time'' construct might have been threatened by the operationalization.

Another construct whose validity might have been threatened is ``size of a code change''.
We operationalize this construct as the number of churned lines, as customary interpreted as the sum of the number of added 
and removed lines~\cite{GigerPG}. 
In this way, moved lines are counted twice, as being added and as being removed.
Moreover, we do not distinguish between lines of Java code and other lines. 
The reason is that while the repositories considered are labeled by GitHub as Java repositories and \emph{predominantly} contain Java code,
they also contain source code in other languages, configuration files and documentation. 
Distinguishing between source code and non-source code in all those languages is non-trivial.

Validity of the measurements we have performed such as the number of commits or the number of builds might have been threatened by the limitations of the GitHub API and Travis API.
Finally, in this work we use terms ``project'' and ``repository'' interchangeably: as pointed out by Kalliamvakou \etal~\cite{Kalliamvakou2014Promises} this is not necessarily the case. 

%is the extent to which the data was collected correctly. We collected the data of commits, issues and PRs from GITHUB. After the data being collected, we recheck to make sure we didn't miss any commits, issues and PRs. For example, we searched the issues using GitHub search API for each project, and compared the returned \textit{total\_count} field with the total number of issues we have collected. If they were different, we found the missing issues, and regathered them until all the issues were gathered. Similarly, the data collected from Travis-CI was also rechecked carefully. To ensure the reliability of test number per build, we randomly selected a sample of builds, manually reviewed the log files and verified the data from manual review and automatical collection to make sure they are consistent. Similar, the data of build error classification was also examined in this manually checking way.

\textbf{Internal Validity}:
Internal validity is the extent to which the conclusions can be drawn from the measurements we have conducted. 
To reduce these threats we have opted for RDD~\cite{imbens2008regression}, a sound approach to statistical modeling of discontinuity in time series. 
Application of RDD to software engineering data has been recently advocated by Wieringa~\cite{Wieringa}.

% about the causal effect of independent variables on the dependent variables. We considered if commit churn, number of tests, build error types and number of issues will evolve in a certain trend after CI is adopted. For each perspective, we filtered the projects based on particular required conditions, and focus on the individual variable, without considering whether other variables will influence the trend. If such variables exist, they may weaken the credibility of our results.
 
\as{Where does the following paragraph belong?} 
In addition, we selected the projects indeed with CI adoption, but didn't take into account the CI usage frequency. If a project has only a few builds within a few years, the software engineering practices may be different from other projects which use CI frequently. 
Besides, in our data set, a group of builds didn't have test execution descriptions in the log files. Most of them are due to the projects without featuring test execution (nearly 30\% projects). For the other builds, the possible reasons are that (1) the log files are blank; (1) the build was canceled before running tests; (3) all the jobs were errored or failed and then exited before running tests; (4)the build was set to skip tests (\eg \textit{DskipTests=true}). To solve this, we selected the projects which featured test execution and had over 90\% builds with at least one test.


\textbf{External Validity}: We performed a large empirical study on 1566 open-source projects. We believe our findings reveal the characteristics of software Engineering Practices following Adoption of Continuous Integration. However we cannot draw absolutely general conclusions on all software.
For example, the projects in our study are all Java open source projects from GitHub repository. Therefore, our conclusion may not be generalized either to projects developed by other programming languages or projects that are not open-source. Besides, as we know, there have been multiple CI techniques in practice, \eg Travis CI, Jenkins CI, Hudson CI. In our study, we only focused on Travis CI, which may also threaten the generalization of our results. However. as Travis-CI is the most popular CI service in GitHub, we believe our results on Travis-CI are representative for CI.  This is an inherent threat for all studies in empirical software engineering, as many factors cannot be characterized completely. To mitigate this threat, there is a need to replicate our study using a wide variety of systems and considering other CI techniques in the future work.
